Directory structure:
└── nanopore-consensus-benchmark/
    ├── README.md
    ├── analysis_interface.ipynb
    ├── data_functions.py
    ├── requirements.txt
    ├── test_book.ipynb
    ├── viz_handler.py
    ├── .dev.blueprint.md
    ├── .dev.outline.md
    ├── results/
    ├── seqs/
    └── summary/

================================================
File: README.md
================================================
# nanopore-consensus-benchmark
Python program to evaluate performance of nanopore amplicon consensus sequence pipelines, presented through a Jupyter notebook



================================================
File: analysis_interface.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Nanopore Consensus Sequence Analysis: Old Guppy Data vs New Dorado Data
## Project Goal
This notebook facilitates a comparative analysis of consensus sequences generated from the same raw Nanopore sequencing data (fungal ITS amplicons) but processed using two different software pipelines; most notably: **Dorado** (the newer method) and **Guppy** (the previous standard).The objective is to quantitatively assess the differences and potential improvements offered by the new Dorado pipeline by comparing key sequence metrics and characteristics.

## Background
The raw Nanopore signal data from fungal ITS sequencing runs (`OMDL*` datasets in our case) was independently processed by both basecallers in multiplex sampple pool. Subsequent steps involved demultiplexing reads into sample-specific bins, clustering similar reads, and generating consensus sequences. The final output for comparison includes FASTA sequence files and associated metadata like "Reads in Consensus" (RiC).  There have been many software improvements implemented since the original Guppy datasets were produced, so this comparison is to investigate the difference produced using the new pipeline on the exact same raw source data. 

## Notebook Workflow
This jupyter notebook provides an interactive interface to:
1.  **Setup and Config:** Load run data, and identify runs with paired Dorado and Guppy data, then and load the corresponding sequence files.
2.  **Match Sequences:** Use some logic to pair corresponding consensus sequences generated by Dorado and Guppy for the *same* original sample.
3.  **Calculate & Compare Metrics:** For matched pairs, calculate and compare key metrics like:
    * Reads in Consensus (RiC)
    * Sequence Length
    * GC Content
    * Sequence Identity (including mismatches, insertions, deletions)
    * Homopolymer run characteristics
    * Frequency of ambiguous bases (not currently relevant)
4.  **Statistical Analysis:** Apply non-parametric tests (e.g., Wilcoxon signed-rank test) to assess the significance of observed differences.
5.  **Visualize Results:** Generate plots (scatter plots, histograms) to visualize comparisons and distributions.
6.  **Explore Alignments:** Interactively view pairwise alignments of matched sequences to examine differences at the base level.
7.  **Summarize & Export:** Generate summary tables for individual runs and across all runs, exporting results to TSV/CSV files.
"""

"""
# Initial Setup and Config
"""

import data_functions
from viz_handler import display_run_analysis
import os
from natsort import natsorted
from IPython.display import display, Markdown, clear_output
import ipywidgets as widgets
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd # type: ignore
import json
# Optional: Reload data_functions if making changes during development
# import importlib
# importlib.reload(data_functions) # Use this in code cell to reload the module

# Configure pandas display
pd.set_option('display.max_columns', None)

# Configure plotting style (optional)
sns.set_theme(style="whitegrid")

# --- Configuration: Define Project Paths ---
# You can change BASE_PROJECT_DIR if your data/results aren't relative to the notebook
BASE_PROJECT_DIR = '.' # Assumes seqs, summary, results are subdirs of the notebook's dir or a linked dir

# Define specific directories relative to the base
SEQS_DIR = os.path.join(BASE_PROJECT_DIR, 'seqs')
SUMMARY_DIR = os.path.join(BASE_PROJECT_DIR, 'summary')
RESULTS_DIR = os.path.join(BASE_PROJECT_DIR, 'results')

# Create results directory if it doesn't exist
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"Using Sequences Directory: {os.path.abspath(SEQS_DIR)}")
print(f"Using Summary Directory:   {os.path.abspath(SUMMARY_DIR)}")
print(f"Using Results Directory:   {os.path.abspath(RESULTS_DIR)}")


"""
### Load Runs
"""

runs_df, runs_dict = data_functions.discover_runs(SEQS_DIR)

print("Discovered Runs:")
display(runs_df)
if runs_dict:
    print("Run dictionary Loaded.")


"""
## Process Runs
Execute the workflow on all runs loaded.
"""

def process_run_data(run_id, seqs_dir, results_dir):
    """Loads, matches, analyzes, and saves data for a single run."""
    print(f"\n--- Processing Run: {run_id} ---")
    run_results = {'run_id': run_id, 'stats': {}, 'counts': {}}

    # 1. Load Sequences
    print("  Loading sequences...")
    dorado_seqs = data_functions.load_sequences(run_id, 'dorado', seqs_dir)
    guppy_seqs = data_functions.load_sequences(run_id, 'guppy', seqs_dir)

    if dorado_seqs is None or guppy_seqs is None:
        print(f"  Skipping {run_id}: Missing Dorado or Guppy sequence file.")
        return None # Indicate failure for this run

    print(f"  Loaded {sum(len(v) for v in dorado_seqs.values())} Dorado sequences across {len(dorado_seqs)} samples.")
    print(f"  Loaded {sum(len(v) for v in guppy_seqs.values())} Guppy sequences across {len(guppy_seqs)} samples.")
    run_results['counts']['dorado_total'] = sum(len(v) for v in dorado_seqs.values()) # Store total counts
    run_results['counts']['guppy_total'] = sum(len(v) for v in guppy_seqs.values())

    # 2. Match Sequences
    print("  Matching sequences...")
    matched_pairs, dorado_only, guppy_only = data_functions.match_sequences(dorado_seqs, guppy_seqs)
    run_results['matched_pairs'] = matched_pairs # Keep matched_pairs for potential later use (e.g., alignment viewer)
    run_results['dorado_only'] = dorado_only
    run_results['guppy_only'] = guppy_only
    run_results['counts']['matched'] = len(matched_pairs)
    run_results['counts']['dorado_only'] = len(dorado_only)
    run_results['counts']['guppy_only'] = len(guppy_only)
    print(f"  Matching complete: {len(matched_pairs)} pairs, {len(dorado_only)} Dorado-only, {len(guppy_only)} Guppy-only.")

    if not matched_pairs:
        print(f"  Skipping analysis for {run_id}: No matched pairs found.")
        return run_results # Return counts even if no matches

    # 3. Consolidate Metrics into DataFrame (Step 3.4)
    print("  Generating comparison DataFrame...")
    run_comparison_df = data_functions.generate_comparison_dataframe(matched_pairs)
    run_results['comparison_df'] = run_comparison_df # Store DF for this run

    if run_comparison_df.empty:
         print(f"  Skipping further analysis for {run_id}: Comparison DataFrame is empty.")
         return run_results

    # 4. Perform Run-Specific Statistical Analysis (Step 4.2)
    print("  Calculating run statistics...")
    run_stats = data_functions.calculate_run_statistics(run_comparison_df)
    run_results['stats'] = run_stats
    print(f"  Statistics calculated: {list(run_stats.keys()) if run_stats else 'None'}")


    # 5. Save Run-Specific Output (Step 4.3)
    print("  Saving run comparison data...")
    saved_path = data_functions.save_run_comparison(run_comparison_df, run_id, results_dir, format='tsv') # Or 'csv'
    if saved_path:
         print(f"  Run comparison saved to: {saved_path}")
         # Optionally save as CSV too
         # data_functions.save_run_comparison(run_comparison_df, run_id, results_dir, format='csv')
    else:
         print("  Failed to save run comparison data.")


    print(f"--- Finished processing {run_id} ---")
    return run_results

# --- Process All Valid Runs ---
all_runs_analysis_results = {}
valid_run_ids = runs_df[runs_df['Both Available']].index.tolist() # Get naturally sorted IDs from the DataFrame

if not valid_run_ids:
    print("No runs found with both Dorado and Guppy data available.")
else:
    print(f"Starting processing for {len(valid_run_ids)} runs: {', '.join(valid_run_ids)}")
    for run_id in valid_run_ids:
         # Pass necessary directory paths to the function
        results = process_run_data(run_id, SEQS_DIR, RESULTS_DIR)
        if results: # Store results even if some steps failed (e.g., no matches)
             all_runs_analysis_results[run_id] = results

    print("\n--- All Run Processing Complete ---")
    print(f"Processed results stored in 'all_runs_analysis_results' dictionary for {len(all_runs_analysis_results)} runs.")

    # Optional: Display a summary of what was processed
    processed_summary = []
    for run_id, data in all_runs_analysis_results.items():
         counts = data.get('counts', {})
         processed_summary.append({
              'Run_ID': run_id,
              'Matched': counts.get('matched', 0),
              'Dorado_Only': counts.get('dorado_only', 0),
              'Guppy_Only': counts.get('guppy_only', 0),
              'Stats_Keys': list(data.get('stats', {}).keys()) if data.get('stats') else 'N/A'
         })
    summary_df = pd.DataFrame(processed_summary)
    print("\nProcessing Summary:")
    display(summary_df)

# --- Generate and Save Overall Summary ---
print("\n--- Generating Overall Summary ---")
if all_runs_analysis_results:
    overall_summary_path = data_functions.generate_overall_summary(
        all_runs_analysis_results,
        RESULTS_DIR,
        format='tsv' # Or 'csv'
    )
    if overall_summary_path:
        print(f"Overall summary saved to: {overall_summary_path}")
        # Optionally load and display the summary DataFrame
        try:
            overall_summary_df = pd.read_csv(overall_summary_path, sep='\t')
            print("\nOverall Summary DataFrame Head:")
            display(overall_summary_df.head())
        except Exception as e:
            print(f"Could not read back overall summary file: {e}")
    else:
        print("Failed to generate overall summary file.")
else:
    print("Skipping overall summary generation: No run results available.")

"""
# Interactive Run Selection
"""

# Get the list of successfully processed run IDs
processed_run_ids = list(all_runs_analysis_results.keys())

# Ensure natural sorting (e.g., OMDL1, OMDL2, OMDL10)
sorted_run_ids = natsorted(processed_run_ids)

if not sorted_run_ids:
    print("Warning: No processed runs available for selection.")
    # Handle case where no runs were processed successfully
    # Maybe display a message and skip widget creation
    # Create the dropdown widget
run_dropdown = widgets.Dropdown(
    options=sorted_run_ids,
    description='Select Run for Detailed Analysis:',
    disabled=not sorted_run_ids, # Disable if no runs are available
    style={'description_width': 'initial'} # Adjust width for longer description
)
# Create an Output widget to hold the results for the selected run
analysis_output_area = widgets.Output()

# Global variable to store the currently selected run ID
# Initialize with the first available run, if any
selected_run_id = sorted_run_ids[0] if sorted_run_ids else None

# Function to be called when the dropdown value changes
def on_run_select_change(change):
    global selected_run_id # Declare intent to modify global variable
    if change['type'] == 'change' and change['name'] == 'value':
        new_run_id = change['new']
        selected_run_id = new_run_id # Update the global variable

        # Clear the output area and display status/trigger analysis
        with analysis_output_area:
            clear_output(wait=True) # Clear previous output before displaying new
            print(f"Selected Run ID: {selected_run_id}")
            # --- Call the main display function ---
            display_run_analysis(selected_run_id, all_runs_analysis_results)


# --- Re-register observer and trigger initial display ---
# (Ensure the dropdown widget and output area are defined before this)

# Unobserve previous function if necessary (good practice if re-running cell)
try:
    run_dropdown.unobserve(on_run_select_change, names='value')
except (AttributeError, KeyError, ValueError): # Handle cases where it wasn't observed before or dropdown doesn't exist
    pass

# Register the new observer function
run_dropdown.observe(on_run_select_change, names='value')

# Display the widgets again if needed (usually done in a separate cell)
display(Markdown("### Select Run for Detailed Analysis:"))
display(run_dropdown)
display(analysis_output_area)

# Manually trigger the display for the initially selected run
# (Important: Make sure 'selected_run_id' has a valid initial value)
if selected_run_id:
     on_run_select_change({'type': 'change', 'name': 'value', 'new': selected_run_id})
else:
     print("No initial run selected or no runs processed.")





================================================
File: data_functions.py
================================================
import os
import re
import glob
import pandas as pd
from collections import defaultdict
from typing import Dict, List, Any, Optional, Tuple, Union
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio import Align
from Bio.SeqUtils import gc_fraction
from scipy import stats
import numpy as np
from natsort import natsorted # For natural sorting of run IDs


def extract_run_info(filename: str) -> Tuple[Optional[str], Optional[str]]:
    """
    Extract run ID (e.g., "OMDL1") and basecaller (e.g., "dorado") from filename.
    Assumes filename format like 'OMDL{number}_seqs_{basecaller}.fasta'.
    """
    # Use regex to capture the number and the basecaller name
    # Pattern: OMDL followed by digits, then _seqs_, then the basecaller name, ending with .fasta
    pattern = r"OMDL(\d+)_seqs_(\w+)\.fasta"
    match = re.search(pattern, filename, re.IGNORECASE) # Use IGNORECASE if dorado/guppy might vary in case
    if match:
        run_number = match.group(1)
        basecaller = match.group(2).lower() # Normalize to lowercase
        run_id = f"OMDL{run_number}"
        # Ensure only expected basecallers are recognized
        if basecaller in ['dorado', 'guppy']:
            return run_id, basecaller
    return None, None

def discover_runs(seqs_dir: str) -> Tuple[pd.DataFrame, dict]:
    """
    Discover all available runs in the seqs directory and check for paired Dorado/Guppy data.

    Args:
        seqs_dir: The path to the directory containing sequence FASTA files.

    Returns:
        A tuple containing:
        - A pandas DataFrame summarizing runs and basecaller availability.
        - A dictionary mapping run IDs to their basecaller status.
    """
    # Define the pattern to search for FASTA files
    pattern = os.path.join(seqs_dir, "OMDL*_seqs_*.fasta")
    seq_files = glob.glob(pattern)

    # Use a dictionary to store run status: {run_id: {'dorado': False, 'guppy': False}}
    all_runs_status = {}

    # Extract run IDs and basecallers from filenames
    for file_path in seq_files:
        filename = os.path.basename(file_path)
        run_id, basecaller = extract_run_info(filename)

        if run_id and basecaller:  # Ensure both were successfully extracted
            # Initialize run_id entry if it's the first time seeing it
            if run_id not in all_runs_status:
                all_runs_status[run_id] = {'dorado': False, 'guppy': False}
            # Update the status for the detected basecaller
            all_runs_status[run_id][basecaller] = True

    # Prepare data for DataFrame conversion
    if all_runs_status:
        # Sort the dictionary by run_id using the natural sort key
        sorted_run_ids = natsorted(all_runs_status.keys())
        sorted_runs_dict = {run_id: all_runs_status[run_id] for run_id in sorted_run_ids}

        # Convert the sorted dictionary to a DataFrame
        runs_df = pd.DataFrame.from_dict(sorted_runs_dict, orient='index')
        runs_df.index.name = 'Run ID'
        # Add the 'Both Available' column
        runs_df['Both Available'] = runs_df['dorado'] & runs_df['guppy']
    else:
        # Create an empty DataFrame with the expected columns if no runs were found
        runs_df = pd.DataFrame(columns=['dorado', 'guppy', 'Both Available'])
        runs_df.index.name = 'Run ID'

    return runs_df, all_runs_status # Return both the DF and the dict

def extract_sample_id(header: str) -> Optional[str]:
    """
    Extract the unique sample identifier (e.g., "OMDL00009") from a FASTA header.
    Example Header: >ONT01.09-A02-OMDL00009-iNat169115711-1 ric=388
    """
    # Regex to find the OMDL part specifically
    # Looks for '-OMDL' followed by digits, capturing the 'OMDL' + digits part
    pattern = r"-(OMDL\d+)"
    match = re.search(pattern, header)
    if match:
        return match.group(1) # Returns "OMDLxxxxx"
    # Fallback or logging if pattern not found, depending on how strict you need to be
    # print(f"Warning: Could not extract OMDL sample ID from header: {header}")
    return None

def parse_ric_value(header: str) -> Optional[int]:
    """
    Extract RiC (Reads in Consensus) value from sequence header. eg ric=388
    """
    pattern = r"ric=(\d+)" # Looks for 'ric=' followed by digits
    match = re.search(pattern, header)
    if match:
        try:
            return int(match.group(1))
        except ValueError:
            # Handle case where digits might be invalid, though unlikely with \d+
            return None
    return None

def load_sequences(run_id: str, basecaller: str, seqs_dir: str) -> Optional[Dict[str, List[Dict[str, Any]]]]:
    """
    Load sequences from a FASTA file for a specific run and basecaller,
    organizing them by sample ID.

    Args:
        run_id: The run identifier (e.g., "OMDL1").
        basecaller: The basecaller name ("dorado" or "guppy").
        seqs_dir: The path to the directory containing sequence FASTA files.

    Returns:
        A dictionary mapping sample IDs (e.g., "OMDL00009") to lists of
        sequence record dictionaries, or None if the file doesn't exist.
        Each sequence dict contains: 'header', 'sequence', 'length', 'ric', 'seq_object'.
    """
    # Construct the full path to the FASTA file
    filename = f"{run_id}_seqs_{basecaller}.fasta"
    filepath = os.path.join(seqs_dir, filename)

    # Check if the file exists before attempting to open
    if not os.path.exists(filepath):
        print(f"Warning: File not found - {filepath}")
        return None

    # Use defaultdict for convenient appending to lists for each sample_id
    sequences_by_sample = defaultdict(list)

    try:
        # Parse the FASTA file using Biopython
        for record in SeqIO.parse(filepath, "fasta"):
            # Extract the unique sample ID (e.g., "OMDLxxxxx")
            sample_id = extract_sample_id(record.description)

            # If a valid sample ID is found, process the record
            if sample_id:
                ric_value = parse_ric_value(record.description)
                sequence_str = str(record.seq)
                sequence_len = len(sequence_str)

                # Store the relevant information in a dictionary
                sequence_data = {
                    'header': record.description,
                    'sequence': sequence_str,
                    'length': sequence_len,
                    'ric': ric_value,
                    # Optionally store the full SeqRecord object if needed for complex BioPython tasks later
                    'seq_object': record
                }
                # Append this sequence's data to the list for its sample ID
                sequences_by_sample[sample_id].append(sequence_data)

    except FileNotFoundError:
        # This case is technically handled by os.path.exists, but good practice
        print(f"Error: File not found during parsing - {filepath}")
        return None
    except Exception as e:
        # Catch other potential errors during file parsing
        print(f"Error parsing FASTA file {filepath}: {e}")
        return None # Or raise the exception depending on desired error handling

    # Return the dictionary (convert defaultdict to dict if preferred, though not necessary)
    return dict(sequences_by_sample)

def calculate_kmer_similarity(seq1: str, seq2: str, k: int = 5) -> float:
    """
    Calculate similarity between two sequences based on shared k-mers.
    Uses counts of k-mers to provide a similarity score.

    Args:
        seq1: The first sequence string.
        seq2: The second sequence string.
        k: The k-mer size (default: 7).

    Returns:
        A similarity score between 0.0 and 100.0, representing the percentage
        of k-mers in seq2 that are also found in seq1 (considering counts).
        Returns 0.0 if either sequence is too short for k-mers or if seq2 has no k-mers.
    """
    len1 = len(seq1)
    len2 = len(seq2)

    # --- Edge Case Handling ---
    # If either sequence is shorter than k, no k-mers can be generated.
    if len1 < k or len2 < k:
        return 0.0

    # --- Generate k-mer counts for seq1 ---
    seq1_kmers = {} # Dictionary to store k-mer counts for seq1
    for i in range(len1 - k + 1):
        kmer = seq1[i:i+k]
        seq1_kmers[kmer] = seq1_kmers.get(kmer, 0) + 1

    # --- Compare k-mers in seq2 ---
    shared_kmers_count = 0
    total_kmers_in_seq2 = len2 - k + 1 # Total k-mers possible in seq2

    # Keep track of k-mers already counted from seq2 to respect counts in seq1
    seq2_kmers_counted = {}

    for i in range(total_kmers_in_seq2):
        kmer = seq2[i:i+k]

        # Check if this k-mer exists in seq1
        if kmer in seq1_kmers:
            # Check how many times we've seen this k-mer in seq2 so far
            current_count_in_seq2 = seq2_kmers_counted.get(kmer, 0)
            # If we haven't counted this k-mer from seq2 more times than it appears in seq1,
            # increment shared count and update counted dictionary for seq2.
            if current_count_in_seq2 < seq1_kmers[kmer]:
                shared_kmers_count += 1
                seq2_kmers_counted[kmer] = current_count_in_seq2 + 1

    # --- Calculate Similarity Score ---
    if total_kmers_in_seq2 == 0:
        return 0.0 # Avoid division by zero if seq2 has no k-mers (though handled by length check)

    similarity = (shared_kmers_count / total_kmers_in_seq2) * 100.0
    return similarity

def align_sequences(seq1: str, seq2: str) -> Optional[Dict[str, Any]]:
    """
    Performs global pairwise alignment of two sequences using Bio.Align.PairwiseAligner
    and calculates alignment metrics.

    Args:
        seq1: The first sequence string.
        seq2: The second sequence string.

    Returns:
        A dictionary containing alignment metrics:
        {'identity': float, 'mismatches': int, 'insertions': int, 'deletions': int,
         'alignment_length': int, 'score': float, 'alignment_obj': Bio.Align.Alignment object}
        or None if alignment fails or sequences are empty.
        'insertions' are gaps in seq1 relative to seq2.
        'deletions' are gaps in seq2 relative to seq1.
    """
    # Handle empty sequences
    if not seq1 or not seq2:
        return None

    # --- Configure the Aligner ---
    aligner = Align.PairwiseAligner()
    aligner.mode = 'global' # Global alignment (Needleman-Wunsch)

    # --- !!! CHANGE SCORING HERE !!! ---
    # Penalize mismatches and gaps
    # Example scores (these can be tuned):
    aligner.match_score = 1.0   # Score for a match (default is 1.0)
    aligner.mismatch_score = -1.0 # usually negative of match_score, possibly x2
    aligner.open_gap_score = -2 # Penalty for opening a gap (default is -2.0); should be larger than extend_gap_score
    aligner.extend_gap_score = -1 # Penalty for extending a gap (default is -1.0)


    try:
        # --- Perform Alignment ---
        # aligner.align returns an iterator; get the best one (or first if scores are simple)
        # Using next() is efficient to get just the first/best result
        alignment = next(aligner.align(seq1, seq2), None)

    except OverflowError:
        # Handle cases where alignment complexity is too high [cite: 24]
        print(f"Warning: Alignment OverflowError for sequences of length {len(seq1)} and {len(seq2)}. Skipping alignment.")
        return None # Indicate failure
    except Exception as e:
        # Catch other potential alignment errors
        print(f"Warning: Alignment failed for sequences of length {len(seq1)} and {len(seq2)}: {e}")
        return None # Indicate failure

    # Check if an alignment was found
    if alignment is None:
        # This might happen if sequences are extremely dissimilar with heavy penalties,
        # though unlikely with 0 penalties.
        return None

    # --- Calculate Metrics Directly from Alignment Object ---
    # Biopython's alignment object allows direct comparison of aligned sequences
    aligned_seq1, aligned_seq2 = alignment[0], alignment[1]
    alignment_length = len(aligned_seq1)

    if alignment_length == 0: # Should not happen if sequences were not empty, but check.
         return None

    matches = 0
    mismatches = 0
    insertions = 0 # Gaps in seq1 ('-')
    deletions = 0  # Gaps in seq2 ('-')

    for char1, char2 in zip(aligned_seq1, aligned_seq2):
        if char1 == char2:
            matches += 1
        elif char1 == '-':
            insertions += 1
        elif char2 == '-':
            deletions += 1
        else:
            mismatches += 1

    # Calculate percentage identity
    identity_percent = (matches / alignment_length) * 100.0

    # Store results in a dictionary
    results = {
        'identity': identity_percent,
        'mismatches': mismatches,
        'insertions': insertions, # gaps in seq1
        'deletions': deletions,   # gaps in seq2
        'alignment_length': alignment_length,
        'score': alignment.score,
        'alignment_obj': alignment # Store the object if needed later (e.g., for visualization)
    }
    return results

def match_sequences(
    dorado_seqs: Dict[str, List[Dict[str, Any]]],
    guppy_seqs: Dict[str, List[Dict[str, Any]]]
) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    Matches sequences between Dorado and Guppy datasets for each sample,
    using k-mer similarity and pairwise alignment.

    Args:
        dorado_seqs: Dictionary mapping sample IDs to lists of Dorado sequence records.
                     (Output of load_sequences).
        guppy_seqs: Dictionary mapping sample IDs to lists of Guppy sequence records.
                    (Output of load_sequences).

    Returns:
        A tuple containing three lists:
        - matched_pairs: List of dictionaries, each representing a matched pair.
                         Includes sample_id, dorado record, guppy record, alignment results,
                         multiple_matches flag, and match_confidence.
        - dorado_only: List of dictionaries for Dorado sequences with no match found.
                       Includes sample_id and the dorado record.
        - guppy_only: List of dictionaries for Guppy sequences with no match found.
                      Includes sample_id and the guppy record.
    """
    matched_pairs = []
    dorado_only = []
    guppy_only = []

    # --- Configuration Thresholds ---
    KMER_SIMILARITY_THRESHOLD = 50.0  # Min k-mer similarity (%) for considering alignment
    LENGTH_RATIO_THRESHOLD = 0.5     # Min length ratio (shorter/longer) to consider match
    HIGH_IDENTITY_THRESHOLD = 95.0    # Identity (%) for high-confidence 1:1 match
    MULTIPLE_MATCH_IDENTITY_DIFF = 5.0 # Max identity % difference for considering ambiguous matches
    # Maximum number of alignments to perform per sample in complex cases to limit computation
    MAX_ALIGNMENTS_PER_SAMPLE = 20

    # Get sets of sample IDs present in each dataset
    dorado_sample_ids = set(dorado_seqs.keys())
    guppy_sample_ids = set(guppy_seqs.keys())

    # Identify common samples, and samples unique to each dataset
    common_samples = dorado_sample_ids.intersection(guppy_sample_ids)
    dorado_unique_samples = dorado_sample_ids - guppy_sample_ids
    guppy_unique_samples = guppy_sample_ids - dorado_sample_ids

    print(f"Processing {len(common_samples)} samples common to both Dorado and Guppy...")

    # --- Process Common Samples ---
    for sample_id in common_samples:
        dorado_records = dorado_seqs[sample_id]
        guppy_records = guppy_seqs[sample_id]
        num_dorado = len(dorado_records)
        num_guppy = len(guppy_records)

        # Keep track of used sequence indices within this sample
        used_dorado_indices = set()
        used_guppy_indices = set()

        # === Case 1: Simple 1-to-1 Match ===
        if num_dorado == 1 and num_guppy == 1:
            d_record = dorado_records[0]
            g_record = guppy_records[0]
            alignment_results = align_sequences(d_record['sequence'], g_record['sequence']) # Step 2.2
            MIN_IDENTITY_THRESHOLD_1_TO_1 = 70.0 # Example threshold, adjust if needed
            if alignment_results and alignment_results['identity'] >= MIN_IDENTITY_THRESHOLD_1_TO_1:
                matched_pairs.append({
                    'sample_id': sample_id,
                    'dorado': d_record,
                    'guppy': g_record,
                    'alignment': alignment_results,
                    'multiple_matches': False,
                    'match_confidence': 'high' if alignment_results['identity'] >= HIGH_IDENTITY_THRESHOLD else ('medium' if alignment_results['identity'] >= 80 else 'low')
                })
                used_dorado_indices.add(0)
                used_guppy_indices.add(0)
            else:
                # Alignment failed, or identity too low. Treat as unmatched.
                pass # They will be added to _only lists later

        # === Case 2: Complex Match (Multiple Sequences in Dorado or Guppy or Both) ===
        elif num_dorado > 0 and num_guppy > 0:
            potential_pair_scores = [] # Store tuples: (d_idx, g_idx, kmer_score)

            # 1. Pre-filter pairs using k-mer similarity and length ratio
            for d_idx, d_record in enumerate(dorado_records):
                for g_idx, g_record in enumerate(guppy_records):
                    len1, len2 = d_record['length'], g_record['length']
                    if min(len1, len2) <= 0: continue # Skip empty sequences  # noqa: E701
                    length_ratio = min(len1, len2) / max(len1, len2)

                    if length_ratio >= LENGTH_RATIO_THRESHOLD:
                        kmer_sim = calculate_kmer_similarity(d_record['sequence'], g_record['sequence']) # Step 2.1

                        if kmer_sim >= KMER_SIMILARITY_THRESHOLD:
                            potential_pair_scores.append((d_idx, g_idx, kmer_sim))

            if not potential_pair_scores:
                 # No pairs passed pre-filtering, all sequences are unmatched for this sample
                 pass # They will be added to _only lists later

            else:
                # 2. Perform full alignment on promising pairs
                potential_pair_scores.sort(key=lambda x: x[2], reverse=True) # Sort by k-mer score DESC
                aligned_pairs = [] # Store tuples: (d_idx, g_idx, alignment_results)
                alignment_cache = {} # Cache results: {(d_idx, g_idx): alignment_results}

                pairs_to_align = potential_pair_scores[:MAX_ALIGNMENTS_PER_SAMPLE] # Limit alignments


                for d_idx, g_idx, kmer_score in pairs_to_align:
                    if (d_idx, g_idx) not in alignment_cache:
                         alignment_results = align_sequences(dorado_records[d_idx]['sequence'], guppy_records[g_idx]['sequence'])
                         alignment_cache[(d_idx, g_idx)] = alignment_results # Cache even if None

                    alignment_results = alignment_cache[(d_idx, g_idx)]
                    if alignment_results: # Only proceed if alignment was successful
                        aligned_pairs.append((d_idx, g_idx, alignment_results))

                # 3. Assign matches based on alignment identity
                if aligned_pairs:
                    aligned_pairs.sort(key=lambda x: x[2]['identity'], reverse=True) # Sort by identity DESC

                    # First pass: Assign high-confidence unique matches
                    for d_idx, g_idx, align_res in aligned_pairs:
                        if d_idx not in used_dorado_indices and g_idx not in used_guppy_indices:
                            if align_res['identity'] >= HIGH_IDENTITY_THRESHOLD:
                                matched_pairs.append({
                                    'sample_id': sample_id,
                                    'dorado': dorado_records[d_idx],
                                    'guppy': guppy_records[g_idx],
                                    'alignment': align_res,
                                    'multiple_matches': False,
                                    'match_confidence': 'high'
                                })
                                used_dorado_indices.add(d_idx)
                                used_guppy_indices.add(g_idx)

                    # Second pass: Handle remaining sequences and potential ambiguities
                    # Group remaining possible matches by dorado index
                    remaining_potentials = defaultdict(list)
                    for d_idx, g_idx, align_res in aligned_pairs:
                         if d_idx not in used_dorado_indices and g_idx not in used_guppy_indices:
                             remaining_potentials[d_idx].append({'g_idx': g_idx, 'identity': align_res['identity']})

                    for d_idx, possible_matches in remaining_potentials.items():
                         if not possible_matches: continue # Should not happen based on logic, but safe check  # noqa: E701

                         # Sort this dorado seq's possible guppy matches by identity
                         possible_matches.sort(key=lambda x: x['identity'], reverse=True)
                         best_match = possible_matches[0]
                         best_g_idx = best_match['g_idx']
                         best_identity = best_match['identity']

                         # Find other matches within the identity difference threshold
                         ambiguous_matches = [best_match]
                         for match in possible_matches[1:]:
                             if best_identity - match['identity'] <= MULTIPLE_MATCH_IDENTITY_DIFF:
                                 ambiguous_matches.append(match)
                             else:
                                 break # Since they are sorted

                         # Assign match(es)
                         if len(ambiguous_matches) == 1:
                             # Single clear best match for this dorado seq among remaining
                             g_idx = best_g_idx
                             align_res = alignment_cache.get((d_idx, g_idx))
                             if align_res: # Should exist
                                 matched_pairs.append({
                                     'sample_id': sample_id,
                                     'dorado': dorado_records[d_idx],
                                     'guppy': guppy_records[g_idx],
                                     'alignment': align_res,
                                     'multiple_matches': False,
                                     'match_confidence': 'medium' if best_identity >= 80 else 'low'
                                 })
                                 used_dorado_indices.add(d_idx)
                                 used_guppy_indices.add(g_idx)
                         else:
                             # Ambiguous case: Multiple guppy seqs match this dorado seq similarly well
                             for match in ambiguous_matches:
                                 g_idx = match['g_idx']
                                 # Check again if guppy index was used by another ambiguous match in this loop
                                 if g_idx not in used_guppy_indices:
                                     align_res = alignment_cache.get((d_idx, g_idx))
                                     if align_res:
                                         matched_pairs.append({
                                             'sample_id': sample_id,
                                             'dorado': dorado_records[d_idx],
                                             'guppy': guppy_records[g_idx],
                                             'alignment': align_res,
                                             'multiple_matches': True, # Flag as ambiguous
                                             'match_confidence': 'ambiguous',
                                             'similarity_to_best': (match['identity'] / best_identity * 100.0) if best_identity > 0 else 0
                                         })
                                         used_guppy_indices.add(g_idx) # Mark guppy seq as used
                             # Mark dorado seq as used after processing all its ambiguous matches
                             used_dorado_indices.add(d_idx)


        # --- Add any remaining unused sequences for this common sample to _only lists ---
        for d_idx, d_record in enumerate(dorado_records):
            if d_idx not in used_dorado_indices:
                dorado_only.append({'sample_id': sample_id, 'record': d_record})
        for g_idx, g_record in enumerate(guppy_records):
            if g_idx not in used_guppy_indices:
                guppy_only.append({'sample_id': sample_id, 'record': g_record})

    # --- Add sequences from samples unique to one dataset ---
    print(f"Processing {len(dorado_unique_samples)} samples unique to Dorado...")
    for sample_id in dorado_unique_samples:
        for record in dorado_seqs[sample_id]:
            dorado_only.append({'sample_id': sample_id, 'record': record})

    print(f"Processing {len(guppy_unique_samples)} samples unique to Guppy...")
    for sample_id in guppy_unique_samples:
        for record in guppy_seqs[sample_id]:
            guppy_only.append({'sample_id': sample_id, 'record': record})

    print(f"Matching complete. Found {len(matched_pairs)} matched pairs, "
          f"{len(dorado_only)} Dorado-only sequences, {len(guppy_only)} Guppy-only sequences.")

    return matched_pairs, dorado_only, guppy_only

def calculate_gc_content(sequence_str: str) -> Optional[float]:
    """
    Calculates the GC content (fraction) of a DNA sequence string.

    Args:
        sequence_str: The DNA sequence as a string.

    Returns:
        The GC content as a float (fraction between 0.0 and 1.0),
        or None if the input is invalid or calculation fails.
    """
    if not isinstance(sequence_str, str) or not sequence_str:
        # Handle non-string or empty input
        return None
    try:
        # Calculate GC fraction. Biopython's gc_fraction handles 'N' and other non-ATGC chars appropriately.
        # It returns a value between 0 and 1.
        return gc_fraction(sequence_str)
    except Exception as e:
        # Catch potential errors during calculation
        print(f"Warning: Could not calculate GC content for sequence snippet '{sequence_str[:20]}...': {e}")
        return None

def analyze_homopolymers(sequence_str: str, min_len: int = 5) -> Optional[Dict[str, Any]]:
    """
    Analyzes a sequence for homopolymer runs of a minimum length.

    Args:
        sequence_str: The DNA sequence as a string.
        min_len: The minimum length of a homopolymer run to report (default: 5).

    Returns:
        A dictionary summarizing homopolymer runs:
        {
            'A': [list of lengths of A runs >= min_len],
            'C': [list of lengths of C runs >= min_len],
            'G': [list of lengths of G runs >= min_len],
            'T': [list of lengths of T runs >= min_len],
            'total_count': Total number of homopolymer runs found,
            'max_len': Maximum length of any homopolymer run found
        }
        or None if the input is invalid.
    """
    if not isinstance(sequence_str, str) or not sequence_str or not isinstance(min_len, int) or min_len < 1:
        return None # Invalid input

    results = {'A': [], 'C': [], 'G': [], 'T': [], 'total_count': 0, 'max_len': 0}
    bases = ['A', 'T', 'C', 'G'] # Case-insensitive matching usually desired

    for base in bases:
        # Regex to find runs of 'base' with length >= min_len (case-insensitive)
        pattern = re.compile(f"({base}{{{min_len},}})", re.IGNORECASE)
        for match in pattern.finditer(sequence_str):
            run_len = len(match.group(1))
            # Store based on the actual base found (upper case)
            actual_base = match.group(1)[0].upper()
            if actual_base in results: # Should always be true for ATCG
                 results[actual_base].append(run_len)
                 results['total_count'] += 1
                 if run_len > results['max_len']:
                      results['max_len'] = run_len

    return results

# Define standard IUPAC ambiguity codes (excluding A, C, G, T)
IUPAC_AMBIGUITY_CODES = "RYMKWSBVDHN"

def analyze_ambiguity(sequence_str: str) -> Optional[Dict[str, Any]]:
    """
    Analyzes a sequence for the count and frequency of IUPAC ambiguity codes.

    Args:
        sequence_str: The DNA sequence as a string.

    Returns:
        A dictionary summarizing ambiguity codes:
        {
            'total_count': Total number of ambiguity characters found,
            'frequency': Total frequency (total_count / sequence_length),
            'counts_per_code': {code: count for each code found}
        }
        or None if the input is invalid or sequence is empty. Returns 0 counts/frequency
        if no ambiguity codes are found.
    """
    if not isinstance(sequence_str, str):
        return None # Invalid input type

    seq_len = len(sequence_str)
    if seq_len == 0:
        return None # Cannot calculate frequency for empty sequence

    total_ambiguity_count = 0
    counts_per_code = {}

    # Use regex for efficient counting (case-insensitive)
    # Creates a pattern like [RYMKWSBVDHN]
    pattern = re.compile(f"([{IUPAC_AMBIGUITY_CODES}])", re.IGNORECASE)

    for match in pattern.finditer(sequence_str):
        code = match.group(1).upper() # Get the matched code, ensure uppercase
        counts_per_code[code] = counts_per_code.get(code, 0) + 1
        total_ambiguity_count += 1

    frequency = total_ambiguity_count / seq_len

    return {
        'total_count': total_ambiguity_count,
        'frequency': frequency,
        'counts_per_code': counts_per_code
    }

def generate_comparison_dataframe(matched_pairs: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Generates a pandas DataFrame containing consolidated metrics for matched sequence pairs.

    Args:
        matched_pairs: A list of dictionaries, where each dictionary represents
                       a matched pair from the match_sequences function.

    Returns:
        A pandas DataFrame with columns for sample ID, headers, RiC, length, GC content,
        alignment metrics, homopolymer stats, ambiguity stats, and differences.
    """
    comparison_data = []

    if not matched_pairs:
        # Return an empty DataFrame with expected columns if no pairs are provided
        # Define expected columns structure here if needed, or return empty DF
        return pd.DataFrame()

    for pair in matched_pairs:
        row_data = {}

        # --- Basic Info ---
        row_data['Sample_ID'] = pair.get('sample_id', None)
        dorado_rec = pair.get('dorado', {})
        guppy_rec = pair.get('guppy', {})
        row_data['Dorado_Header'] = dorado_rec.get('header', None)
        row_data['Guppy_Header'] = guppy_rec.get('header', None)
        row_data['Dorado_RiC'] = dorado_rec.get('ric', None)
        row_data['Guppy_RiC'] = guppy_rec.get('ric', None)
        row_data['Dorado_Length'] = dorado_rec.get('length', 0) # Use 0 for calculations if missing
        row_data['Guppy_Length'] = guppy_rec.get('length', 0)

        # --- Match Quality ---
        row_data['Multiple_Matches'] = pair.get('multiple_matches', False)
        row_data['Match_Confidence'] = pair.get('match_confidence', None)

        # --- Alignment Metrics (from pair['alignment']) ---
        alignment_res = pair.get('alignment', {})
        row_data['Identity_Percent'] = alignment_res.get('identity', None)
        row_data['Mismatches'] = alignment_res.get('mismatches', None)
        row_data['Insertions_vs_Guppy'] = alignment_res.get('insertions', None) # Gaps in Dorado seq
        row_data['Deletions_vs_Guppy'] = alignment_res.get('deletions', None)   # Gaps in Guppy seq
        row_data['Alignment_Length'] = alignment_res.get('alignment_length', None)
        row_data['Alignment_Score'] = alignment_res.get('score', None)


        # --- Calculated Metrics ---
        dorado_seq = dorado_rec.get('sequence', "")
        guppy_seq = guppy_rec.get('sequence', "")

        # GC Content (handle None return)
        row_data['Dorado_GC'] = calculate_gc_content(dorado_seq)
        row_data['Guppy_GC'] = calculate_gc_content(guppy_seq)

        # Homopolymers (store key results, handle None return)
        dorado_homo = analyze_homopolymers(dorado_seq)
        guppy_homo = analyze_homopolymers(guppy_seq)
        row_data['Dorado_Homo_Count'] = dorado_homo['total_count'] if dorado_homo else None
        row_data['Guppy_Homo_Count'] = guppy_homo['total_count'] if guppy_homo else None
        row_data['Dorado_Homo_MaxLen'] = dorado_homo['max_len'] if dorado_homo else None
        row_data['Guppy_Homo_MaxLen'] = guppy_homo['max_len'] if guppy_homo else None
        # Optionally store the full dicts:
        # row_data['Dorado_Homo_Details'] = dorado_homo
        # row_data['Guppy_Homo_Details'] = guppy_homo

        # Ambiguity (store key results, handle None return)
        dorado_ambig = analyze_ambiguity(dorado_seq)
        guppy_ambig = analyze_ambiguity(guppy_seq)
        row_data['Dorado_Ambig_Count'] = dorado_ambig['total_count'] if dorado_ambig else None
        row_data['Guppy_Ambig_Count'] = guppy_ambig['total_count'] if guppy_ambig else None
        row_data['Dorado_Ambig_Freq'] = dorado_ambig['frequency'] if dorado_ambig else None
        row_data['Guppy_Ambig_Freq'] = guppy_ambig['frequency'] if guppy_ambig else None
        # Optionally store the full dicts:
        # row_data['Dorado_Ambig_Details'] = dorado_ambig
        # row_data['Guppy_Ambig_Details'] = guppy_ambig

        # --- Difference Metrics (handle None values carefully) ---
        try:
            row_data['RiC_Difference'] = (row_data['Dorado_RiC'] - row_data['Guppy_RiC']) if row_data['Dorado_RiC'] is not None and row_data['Guppy_RiC'] is not None else None
        except TypeError:
            row_data['RiC_Difference'] = None
        try:
            row_data['Length_Difference'] = row_data['Dorado_Length'] - row_data['Guppy_Length']
        except TypeError:
             row_data['Length_Difference'] = None
        try:
            row_data['GC_Difference'] = (row_data['Dorado_GC'] - row_data['Guppy_GC']) if row_data['Dorado_GC'] is not None and row_data['Guppy_GC'] is not None else None
        except TypeError:
             row_data['GC_Difference'] = None

        comparison_data.append(row_data)

    # Create DataFrame
    run_comparison_df = pd.DataFrame(comparison_data)

    return run_comparison_df

def perform_paired_nonparametric_test(
    data1: Union[List[float], np.ndarray],
    data2: Union[List[float], np.ndarray],
    test_type: str = 'wilcoxon'
) -> Optional[Tuple[float, float]]:
    """
    Performs a paired non-parametric statistical test between two datasets.

    Args:
        data1: First list or array of paired numerical data.
        data2: Second list or array of paired numerical data. Must be the same length as data1.
        test_type: The type of non-parametric test to perform. Currently supports 'wilcoxon'.

    Returns:
        A tuple containing the test statistic and the p-value,
        or None if the test cannot be performed (e.g., insufficient data, inputs invalid).
    """
    # Basic validation: Check if inputs are lists or numpy arrays
    if not isinstance(data1, (list, np.ndarray)) or not isinstance(data2, (list, np.ndarray)):
        print("Warning: Inputs must be lists or numpy arrays.")
        return None

    # Convert to numpy arrays for easier handling
    data1 = np.asarray(data1)
    data2 = np.asarray(data2)

    # Check for equal length
    if len(data1) != len(data2):
        print("Warning: Input data lists must have the same length.")
        return None

    # Check for sufficient data points (Wilcoxon needs at least a few pairs)
    # SciPy's Wilcoxon handles zero differences, but raises ValueError if data is identical
    # or length is very small. Let's add a basic length check.
    min_required_pairs = 1 # Wilcoxon technically runs with 1, but warns for small N.
    if len(data1) < min_required_pairs:
         print(f"Warning: Insufficient data for {test_type} test (requires at least {min_required_pairs} pairs).")
         return None

    # Check if data is numeric (redundant if type hints are enforced, but good practice)
    if not np.issubdtype(data1.dtype, np.number) or not np.issubdtype(data2.dtype, np.number):
         print("Warning: Input data must be numeric.")
         return None

    # Check for NaN values - Wilcoxon might handle them depending on version/options,
    # but it's often better to remove pairs with NaN explicitly or handle upstream.
    # For simplicity here, we'll let scipy handle it but be aware.
    # Consider adding:
    # combined_data = np.vstack((data1, data2)).T
    # combined_data = combined_data[~np.isnan(combined_data).any(axis=1)]
    # if len(combined_data) < min_required_pairs: return None
    # data1, data2 = combined_data[:, 0], combined_data[:, 1]

    if test_type.lower() == 'wilcoxon':
        try:
            # Calculate differences, ignoring pairs where difference is zero for Wilcoxon
            diff = data1 - data2
            diff_nonzero = diff[diff != 0]

            # If all differences are zero, the test is not applicable / p-value is 1
            if len(diff_nonzero) == 0:
                 print("Warning: All differences are zero, Wilcoxon test not applicable (p=1.0 assumed).")
                 return 0.0, 1.0 # Or return None, depending on desired handling

            # Perform the Wilcoxon signed-rank test
            # Use alternative='two-sided' for standard comparison
            # zero_method='wilcox' is default, handles zeros appropriately
            # correction=False is default, set True for continuity correction if desired
            statistic, p_value = stats.wilcoxon(data1, data2, zero_method='wilcox', alternative='two-sided')

            return statistic, p_value

        except ValueError as ve:
            # Handle specific errors, e.g., insufficient data after removing zeros
            print(f"Error during Wilcoxon test: {ve}")
            return None
        except Exception as e:
            # Catch any other unexpected errors
            print(f"An unexpected error occurred during the {test_type} test: {e}")
            return None
    else:
        print(f"Error: Unsupported test type '{test_type}'. Only 'wilcoxon' is implemented.")
        return None

def calculate_run_statistics(run_df: pd.DataFrame) -> Optional[Dict[str, Any]]:
    """
    Calculates descriptive statistics and performs paired non-parametric tests
    on the comparison DataFrame for a single run.

    Args:
        run_df: The DataFrame containing paired comparison data for one run
                (output of generate_comparison_dataframe).

    Returns:
        A dictionary containing statistics (median differences, p-values, etc.)
        for key metrics, or None if input is invalid or empty.
    """
    # Inside calculate_run_statistics function:

    if not isinstance(run_df, pd.DataFrame) or run_df.empty:
        print("Warning: Input must be a non-empty pandas DataFrame.")
        return None

    # Define the key metric pairs to analyze
    metric_pairs = {
        'RiC': ('Dorado_RiC', 'Guppy_RiC'),
        'Length': ('Dorado_Length', 'Guppy_Length'),
        'GC': ('Dorado_GC', 'Guppy_GC'),
        'Homo_Count': ('Dorado_Homo_Count', 'Guppy_Homo_Count'),
        'Homo_MaxLen': ('Dorado_Homo_MaxLen', 'Guppy_Homo_MaxLen'),
        'Ambig_Count': ('Dorado_Ambig_Count', 'Guppy_Ambig_Count'),
        'Ambig_Freq': ('Dorado_Ambig_Freq', 'Guppy_Ambig_Freq')
    }

    results = {} # Dictionary to store all results

    # Check if required columns exist
    required_cols = [col for pair in metric_pairs.values() for col in pair]
    if not all(col in run_df.columns for col in required_cols):
         missing = [col for col in required_cols if col not in run_df.columns]
         print(f"Warning: DataFrame missing required columns: {missing}. Cannot perform all statistics.")
         # You might choose to return None or proceed with available columns
         # For now, let's proceed and handle missing columns per metric
    # Inside calculate_run_statistics function (after validation):
    for metric_name, (col1, col2) in metric_pairs.items():
        results[metric_name] = {} # Sub-dictionary for each metric

        # Check if both columns for this metric exist
        if col1 not in run_df.columns or col2 not in run_df.columns:
            print(f"Skipping statistics for {metric_name} due to missing columns.")
            results[metric_name]['error'] = "Missing columns"
            continue

        # Extract data, dropping rows where either value is NaN for this pair
        valid_data = run_df[[col1, col2]].dropna()

        if valid_data.empty:
             print(f"No valid data pairs for {metric_name} after dropping NaN.")
             results[metric_name]['error'] = "No valid data"
             continue

        data1 = valid_data[col1]
        data2 = valid_data[col2]

        # Calculate Differences (Data1 - Data2, e.g., Dorado - Guppy)
        differences = data1 - data2

        # --- Descriptive Statistics for Differences ---
        results[metric_name]['median_diff'] = np.median(differences)
        results[metric_name]['mean_diff'] = np.mean(differences)
        # Calculate IQR (Interquartile Range)
        q1 = np.percentile(differences, 25)
        q3 = np.percentile(differences, 75)
        results[metric_name]['iqr_diff'] = q3 - q1
        results[metric_name]['n_pairs'] = len(valid_data) # Number of pairs used

        # --- Perform Paired Non-parametric Test ---
        test_result = perform_paired_nonparametric_test(data1.to_list(), data2.to_list(), test_type='wilcoxon')

        if test_result:
            statistic, p_value = test_result
            results[metric_name]['test_statistic'] = statistic
            results[metric_name]['p_value'] = p_value
        else:
            results[metric_name]['test_statistic'] = None
            results[metric_name]['p_value'] = None
            results[metric_name]['error'] = results[metric_name].get('error', 'Test failed or insufficient data') # Keep previous error if exists

    return results

def save_run_comparison(
    run_df: pd.DataFrame,
    run_id: str,
    output_dir: str,
    format: str = 'tsv'
) -> Optional[str]:
    """
    Saves the run-specific comparison DataFrame to a file (TSV or CSV).

    Args:
        run_df: The DataFrame containing detailed comparison data for matched pairs.
        run_id: The run identifier (e.g., "OMDL1").
        output_dir: The path to the directory where the file will be saved.
        format: The output file format ('tsv' or 'csv'). Defaults to 'tsv'.

    Returns:
        The full path to the saved file, or None if saving fails.
    """
    if not isinstance(run_df, pd.DataFrame):
        print("Error: Input 'run_df' must be a pandas DataFrame.")
        return None
    if run_df.empty:
        print("Warning: Input DataFrame is empty. No file will be saved.")
        return None
    if not run_id or not isinstance(run_id, str):
        print("Error: Invalid 'run_id'.")
        return None
    if not output_dir or not isinstance(output_dir, str):
         print("Error: Invalid 'output_dir'.")
         return None

    # Determine separator and file extension based on format
    if format.lower() == 'csv':
        separator = ','
        file_extension = 'csv'
    elif format.lower() == 'tsv':
        separator = '\t'
        file_extension = 'tsv'
    else:
        print(f"Error: Unsupported format '{format}'. Use 'tsv' or 'csv'.")
        return None

    # Construct filename and full path
    filename = f"{run_id}_comparison_data.{file_extension}"
    filepath = os.path.join(output_dir, filename)

    # Ensure output directory exists (optional, could rely on notebook setup)
    try:
        os.makedirs(output_dir, exist_ok=True)
    except OSError as e:
         print(f"Error creating output directory {output_dir}: {e}")
         return None

    try:
        run_df.to_csv(filepath, sep=separator, index=False, encoding='utf-8')
        print(f"Run comparison data saved to: {filepath}")
        return filepath
    except Exception as e:
        print(f"Error saving DataFrame to {filepath}: {e}")
        return None

def generate_overall_summary(
    all_runs_results: Dict[str, Dict[str, Any]],
    output_dir: str,
    format: str = 'tsv'
) -> Optional[str]:
    """
    Generates an overall summary DataFrame aggregating key statistics and counts
    across all processed runs and saves it to a file.

    Args:
        all_runs_results: A dictionary where keys are run IDs and values are
                          dictionaries containing processed data for each run.
                          Expected structure per run_id:
                          {
                              'stats': dict_output_from_calculate_run_statistics,
                              'counts': {'matched': int, 'dorado_only': int, 'guppy_only': int}
                              # Add other relevant top-level counts if needed, e.g., total sequences
                          }
        output_dir: The path to the directory where the summary file will be saved.
        format: The output file format ('tsv' or 'csv'). Defaults to 'tsv'.

    Returns:
        The full path to the saved summary file, or None if saving fails or input is invalid.
    """
    if not isinstance(all_runs_results, dict) or not all_runs_results:
        print("Error: Input 'all_runs_results' must be a non-empty dictionary.")
        return None
    if not output_dir or not isinstance(output_dir, str):
         print("Error: Invalid 'output_dir'.")
         return None

    summary_data_list = []
    processed_run_ids = natsorted(all_runs_results.keys()) # Use natural sort for Run IDs

    for run_id in processed_run_ids:
        run_result = all_runs_results[run_id]
        row_data = {'Run_ID': run_id}

        # --- Extract Counts ---
        counts = run_result.get('counts', {})
        row_data['Matched_Pairs'] = counts.get('matched', 0)
        row_data['Dorado_Only_Seqs'] = counts.get('dorado_only', 0)
        row_data['Guppy_Only_Seqs'] = counts.get('guppy_only', 0)
        # Add total sequence counts if available/needed, e.g.:
        # row_data['Dorado_Total_Seqs'] = counts.get('dorado_total', 0)
        # row_data['Guppy_Total_Seqs'] = counts.get('guppy_total', 0)


        # --- Extract Key Statistics (Median Diffs, P-values) ---
        stats = run_result.get('stats', {}) # Get the stats dict from Step 4.2 output

        # Define metrics to extract from the stats dict
        metrics_to_summarize = ['RiC', 'Length', 'GC', 'Homo_Count', 'Homo_MaxLen', 'Ambig_Count']

        for metric in metrics_to_summarize:
            metric_stats = stats.get(metric, {}) # Get sub-dict for this metric
            row_data[f'{metric}_Median_Diff'] = metric_stats.get('median_diff') # Use .get() for safety
            row_data[f'{metric}_p_value'] = metric_stats.get('p_value')
            # Add mean diff or IQR if desired
            # row_data[f'{metric}_Mean_Diff'] = metric_stats.get('mean_diff')
            # row_data[f'{metric}_IQR_Diff'] = metric_stats.get('iqr_diff')
            row_data[f'{metric}_N_Pairs'] = metric_stats.get('n_pairs', 0) # Include number of pairs used for test


        summary_data_list.append(row_data)
    if not summary_data_list:
        print("Warning: No data aggregated for the overall summary. No file saved.")
        return None

    # Create DataFrame
    overall_summary_df = pd.DataFrame(summary_data_list)

    # --- Save the DataFrame ---
    # Determine separator and file extension
    if format.lower() == 'csv':
        separator = ','
        file_extension = 'csv'
    elif format.lower() == 'tsv':
        separator = '\t'
        file_extension = 'tsv'
    else:
        print(f"Error: Unsupported format '{format}'. Use 'tsv' or 'csv'.")
        return None

    # Construct filename and full path
    filename = f"overall_comparison_summary.{file_extension}"
    filepath = os.path.join(output_dir, filename)

    # Ensure output directory exists
    try:
        os.makedirs(output_dir, exist_ok=True)
    except OSError as e:
         print(f"Error creating output directory {output_dir}: {e}")
         return None

    # Save the file
    try:
        overall_summary_df.to_csv(filepath, sep=separator, index=False, encoding='utf-8', float_format='%.4f') # Format floats nicely
        print(f"Overall summary data saved to: {filepath}")
        return filepath
    except Exception as e:
        print(f"Error saving overall summary DataFrame to {filepath}: {e}")
        return None


# Currently unused, but keeping for potential future use
def load_summary(run_id: str, basecaller: str, summary_dir: str) -> Optional[Dict[str, Any]]:
    """
    Load summary data from the TSV-like .txt file for a specific run and basecaller.
    Parses the main data table and extracts summary statistics from the end of the file.

    Args:
        run_id: The run identifier (e.g., "OMDL1").
        basecaller: The basecaller name ("dorado" or "guppy").
        summary_dir: The path to the directory containing summary .txt files.

    Returns:
        A dictionary containing the summary data DataFrame ('data') and
        a dictionary of summary statistics ('stats'), or None if the file doesn't exist.
        Stats dict includes: 'unique_samples', 'consensus_sequences', 'total_ric'.
    """
    # Construct the full path to the summary file (note the .txt extension)
    filename = f"{run_id}_summary_{basecaller}.txt"
    filepath = os.path.join(summary_dir, filename)

    # Check if the file exists before attempting to open
    if not os.path.exists(filepath):
        print(f"Warning: Summary file not found - {filepath}")
        return None

    summary_df = None
    summary_stats = {
        'unique_samples': None,
        'consensus_sequences': None,
        'total_ric': None
    }

    try:
        # Step 1: Read the main data table using pandas
        # Assuming the table starts from the first line (header=0)
        # and ends before the summary lines. pandas might stop reading
        # automatically if the summary lines have a different number of columns,
        # but explicitly handling might be safer if format varies.
        # We'll read the whole file first, then extract summary lines separately.
        summary_df = pd.read_csv(filepath, sep='\t', header=0)

        # Remove potential summary lines that might have been read into the DataFrame
        # Identify rows where the first column doesn't look like a filename (e.g., doesn't start with 'ONT')
        # This assumes filenames always start with 'ONT', adjust if needed based on actual data.
        if not summary_df.empty and summary_df.columns[0] == 'Filename': # Check if first column is 'Filename'
             summary_df = summary_df[summary_df['Filename'].str.startswith('ONT', na=False)]

        # Step 2: Read the file again to reliably extract summary statistics from the end
        with open(filepath, 'r') as f:
            lines = f.readlines()

        for line in lines:
            line = line.strip()
            if not line: # Skip empty lines
                continue

            parts = line.split('\t')
            if len(parts) >= 2:
                key = parts[0].strip()
                value_str = parts[-1].strip() # Take the last part as value

                try:
                    value_int = int(value_str)
                    if "Total Unique Samples" in key:
                        summary_stats['unique_samples'] = value_int
                    elif "Total Consensus Sequences" in key:
                        summary_stats['consensus_sequences'] = value_int
                    elif "Total Reads in Consensus Sequences" in key:
                        summary_stats['total_ric'] = value_int
                except ValueError:
                    # Ignore lines where the value isn't an integer
                    continue

    except FileNotFoundError:
        print(f"Error: Summary file not found during processing - {filepath}")
        return None
    except pd.errors.EmptyDataError:
        print(f"Warning: Summary file is empty - {filepath}")
        # Return dictionary with empty DataFrame and None stats
        return {'data': pd.DataFrame(), 'stats': summary_stats}
    except Exception as e:
        print(f"Error processing summary file {filepath}: {e}")
        return None # Or return partial data if appropriate

    # Check if DataFrame was successfully loaded
    if summary_df is None:
         summary_df = pd.DataFrame() # Ensure a DataFrame is always returned

    return {'data': summary_df, 'stats': summary_stats}


================================================
File: requirements.txt
================================================
pandas
numpy
biopython>=1.80
scipy
matplotlib
seaborn
ipython
pathlib
plotly
ipywidgets
xlsxwriter
requests
openpyxl
tqdm
natsort


================================================
File: test_book.ipynb
================================================
# Jupyter notebook converted to Python script.

import data_functions
from viz_handler import plot_metric_comparison, plot_histogram, plot_comparison_with_difference
import os
from natsort import natsorted
from IPython.display import display, Markdown # type: ignore
import ipywidgets as widgets
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd # type: ignore
import json
# Optional: Reload data_functions if making changes during development
# import importlib
# importlib.reload(data_functions) # Use this in code cell to reload the module

# Configure pandas display
pd.set_option('display.max_columns', None)

# Configure plotting style (optional)
sns.set_theme(style="whitegrid")

# --- Configuration: Define Project Paths ---
# You can change BASE_PROJECT_DIR if your data/results aren't relative to the notebook
BASE_PROJECT_DIR = '.' # Assumes seqs, summary, results are subdirs of the notebook's dir or a linked dir

# Define specific directories relative to the base
SEQS_DIR = os.path.join(BASE_PROJECT_DIR, 'seqs')
SUMMARY_DIR = os.path.join(BASE_PROJECT_DIR, 'summary')
RESULTS_DIR = os.path.join(BASE_PROJECT_DIR, 'results')

# Create results directory if it doesn't exist
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"Using Sequences Directory: {os.path.abspath(SEQS_DIR)}")
print(f"Using Summary Directory:   {os.path.abspath(SUMMARY_DIR)}")
print(f"Using Results Directory:   {os.path.abspath(RESULTS_DIR)}")


"""
## Test loading sequences
"""

test_run_id = 'OMDL1' # Replace with a valid run ID from your data
test_basecaller = 'dorado' # Replace 'dorado' or 'guppy' as available

loaded_sequences = data_functions.load_sequences(test_run_id, test_basecaller, SEQS_DIR)

if loaded_sequences is not None:
    print(f"Successfully loaded data for {test_run_id} {test_basecaller}.")
    print(f"Found data for {len(loaded_sequences)} unique sample IDs.")

    # Example: Inspect data for one sample ID (replace 'OMDLxxxxx' with a real ID)
    example_sample_id = list(loaded_sequences.keys())[0] # Get the first sample ID found
    print(f"\nData for sample ID '{example_sample_id}':")
    # Pretty print the list of sequence dictionaries for this sample
    print(json.dumps(loaded_sequences[example_sample_id], indent=2, default=str)) # Use default=str to handle SeqRecord object if present

    # Verify structure of one sequence entry
    first_seq_data = loaded_sequences[example_sample_id][0]
    print("\nStructure of one sequence entry:")
    print(f"  Header: {first_seq_data.get('header')[:50]}...") # Show first 50 chars
    print(f"  Length: {first_seq_data.get('length')}")
    print(f"  RiC: {first_seq_data.get('ric')}")
    print(f"  Sequence snippet: {first_seq_data.get('sequence')[:50]}...") # Show first 50 chars
else:
    print(f"Failed to load data for {test_run_id} {test_basecaller}. Check file path and format.")

"""
## Test K-mer matching
"""

seq_a = "ATGCGATGCGATGCG"
seq_b = "ATGCGATGCGATGCG" # Identical
seq_c = "ATGCGATTCGATGCG" # One mismatch
seq_d = "AAAAAAAAAAAAAAA" # Different
seq_e = "ATGCG"             # Too short for k=7
seq_f = ""                # Empty

k_val = 7
print(f"Similarity A vs B (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_b, k=k_val):.2f}%")
print(f"Similarity A vs C (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_c, k=k_val):.2f}%") # Test mismatch 22.22%
print(f"Similarity B vs A (k={k_val}): {data_functions.calculate_kmer_similarity(seq_b, seq_a, k=k_val):.2f}%") # Should be symmetric? Test.
print(f"Similarity A vs D (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_d, k=k_val):.2f}%") # Test different sequence 0.00%
print(f"Similarity A vs E (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_e, k=k_val):.2f}%") # Test too short sequence 0.00%
print(f"Similarity A vs F (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_f, k=k_val):.2f}%") # Test empty sequence 0.00%

k_val = 3
print(f"\nSimilarity A vs C (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_c, k=k_val):.2f}%") # Test smaller k, 76.92%
print(f"Similarity A vs E (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_e, k=k_val):.2f}%") # Should work now

"""
## Test Global Alignments
"""

seq_a = "ATGCGATGCGATGCG"
seq_b = "ATGCGATGCGATGCG" # Identical
seq_c = "ATGCGATTCGATGCG" # One mismatch
seq_d = "AAAAAAAAAAAAAAA" # Different
seq_indel = "ATGCGATG---ATGCG" # Example with deletion relative to A

align_ab = data_functions.align_sequences(seq_a, seq_b)
align_ac = data_functions.align_sequences(seq_a, seq_c)
align_ad = data_functions.align_sequences(seq_a, seq_d)
align_a_indel = data_functions.align_sequences(seq_a, seq_indel)

print("Alignment A vs B:")
print(json.dumps(align_ab, indent=2, default=str)) # Use default=str to handle alignment obj if needed

print("\nAlignment A vs C:")
print(json.dumps(align_ac, indent=2, default=str))

print("\nAlignment A vs D:")
print(json.dumps(align_ad, indent=2, default=str))

print("\nAlignment A vs Indel:")
print(json.dumps(align_a_indel, indent=2, default=str))

# Test empty sequence
align_a_empty = data_functions.align_sequences(seq_a, "")
print("\nAlignment A vs Empty:")
print(align_a_empty)

"""
## Sequence Matching Logic
"""

# --- Define Test Sequences ---
seq_a = "ATGCGATGCGATGCG"     # Base sequence
seq_b = "ATGCGATGCGATGCG"     # Identical to A
seq_c = "ATGCGATTCGATGCG"     # One mismatch vs A
seq_d = "AAAAAAAAAAAAAAA"     # Very different from A
seq_indel = "ATGCGATG---ATGCG"  # Contains gaps (Note: align_sequences takes raw seqs, not pre-aligned)
seq_e = "ATGCG"                 # Short sequence
seq_f = ""                    # Empty sequence
# Create sequences similar to D and C for testing many:many and ambiguous cases
seq_d_like = "AAAAAAAAAAAAAAC" # Similar to D
seq_c_prime = "ATGCGATTCAATGCG" # Similar to C (two mismatches vs A)

# --- Helper Function to Create Sequence Records ---
# Mimics the structure produced by load_sequences
def create_record(seq_id: str, sequence: str, ric: int, source: str, sample: str, rep_num: int = 1):
    """Creates a dictionary representing a sequence record."""
    # Create a somewhat realistic header based on inputs
    header = f">ONT01.01-{sample}-{seq_id}-iNat0000{rep_num} ric={ric}"
    return {
        'header': header,
        'sequence': sequence,
        'length': len(sequence),
        'ric': ric,
        'seq_object': None # Placeholder, not needed for matching logic testing
    }

# --- Create Mock Dorado Sequences Dictionary ---
mock_dorado_seqs = {
    # Scenario S1: Simple 1:1 High Identity
    'S1': [create_record('D_S1_1', seq_a, 100, 'dorado', 'S1')],
    # Scenario S2: Simple 1:1 Lower Identity
    'S2': [create_record('D_S2_1', seq_a, 90, 'dorado', 'S2')],
    # Scenario S3: Unmatched Pair
    'S3': [create_record('D_S3_1', seq_a, 80, 'dorado', 'S3')],
    # Scenario S4: Dorado Only Sample
    'S4': [create_record('D_S4_1', seq_a, 70, 'dorado', 'S4')],
    # Scenario S6: 1 Dorado, 2 Guppy
    'S6': [create_record('D_S6_1', seq_a, 110, 'dorado', 'S6')],
    # Scenario S7: 2 Dorado, 2 Guppy (Clear matches)
    'S7': [
        create_record('D_S7_1', seq_a, 120, 'dorado', 'S7', rep_num=1),
        create_record('D_S7_2', seq_d, 50, 'dorado', 'S7', rep_num=2)
    ],
    # Scenario S8: 1 Dorado, 2 Guppy (Ambiguous matches)
    'S8': [create_record('D_S8_1', seq_a, 130, 'dorado', 'S8')],
}

# --- Create Mock Guppy Sequences Dictionary ---
mock_guppy_seqs = {
    # Scenario S1: Simple 1:1 High Identity
    'S1': [create_record('G_S1_1', seq_b, 95, 'guppy', 'S1')],
    # Scenario S2: Simple 1:1 Lower Identity
    'S2': [create_record('G_S2_1', seq_c, 85, 'guppy', 'S2')],
    # Scenario S3: Unmatched Pair
    'S3': [create_record('G_S3_1', seq_d, 75, 'guppy', 'S3')],
    # Scenario S5: Guppy Only Sample
    'S5': [create_record('G_S5_1', seq_a, 65, 'guppy', 'S5')],
    # Scenario S6: 1 Dorado, 2 Guppy
    'S6': [
        create_record('G_S6_1', seq_b, 105, 'guppy', 'S6', rep_num=1), # Should match D_S6_1 well
        create_record('G_S6_2', seq_c, 45, 'guppy', 'S6', rep_num=2)  # Should match D_S6_1 less well
    ],
    # Scenario S7: 2 Dorado, 2 Guppy (Clear matches)
    'S7': [
        create_record('G_S7_1', seq_b, 115, 'guppy', 'S7', rep_num=1), # Should match D_S7_1 (A)
        create_record('G_S7_2', seq_d_like, 55, 'guppy', 'S7', rep_num=2) # Should match D_S7_2 (D)
    ],
    # Scenario S8: 1 Dorado, 2 Guppy (Ambiguous matches)
    'S8': [
        create_record('G_S8_1', seq_c, 125, 'guppy', 'S8', rep_num=1),       # Similar match to D_S8_1 (A)
        create_record('G_S8_2', seq_c_prime, 110, 'guppy', 'S8', rep_num=2) # Also similar match to D_S8_1 (A)
    ],
}

print("Mock data dictionaries created: mock_dorado_seqs, mock_guppy_seqs")
# Print a sample entry to verify structure
example_sample_id = 'S7'
print(f"\nExample entry for {example_sample_id} in mock_dorado_seqs:")
print(json.dumps(mock_dorado_seqs.get(example_sample_id, 'Not Found'), indent=2))
print(f"\nExample entry for {example_sample_id} in mock_guppy_seqs:")
print(json.dumps(mock_guppy_seqs.get(example_sample_id, 'Not Found'), indent=2))

matched, dorado_unmatched, guppy_unmatched = data_functions.match_sequences(mock_dorado_seqs, mock_guppy_seqs)

print(f"Matched pairs: {len(matched)}")
print(f"Dorado-only: {len(dorado_unmatched)}")
print(f"Guppy-only: {len(guppy_unmatched)}")

print("\n--- Matched Pairs ---")
for pair in matched:
    print(f"  Sample: {pair['sample_id']}, "
          f"D_Header: {pair['dorado'].get('header','N/A')}, "
          f"G_Header: {pair['guppy'].get('header','N/A')}, "
          f"Identity: {pair['alignment']['identity']:.2f}%, "
          f"Multiple: {pair['multiple_matches']}, "
          f"Confidence: {pair['match_confidence']}")

print("\n--- Dorado Only ---")
for item in dorado_unmatched:
    print(f"  Sample: {item['sample_id']}, Header: {item['record'].get('header','N/A')}")

print("\n--- Guppy Only ---")
for item in guppy_unmatched:
    print(f"  Sample: {item['sample_id']}, Header: {item['record'].get('header','N/A')}")

"""
## Testing GC Content
"""

# Test cases for GC content
seq1 = "ATGCATGC" # Expected GC: 0.5
seq2 = "AAAAATTTTT" # Expected GC: 0.0
seq3 = "GCGCGCGC" # Expected GC: 1.0
seq4 = "ATGCNNNNATGC" # Expected GC: 0.5 (gc_fraction ignores 'N')
seq5 = "" # Expected: None (or handle as 0.0 if preferred)
seq6 = "ATGC-ATGC" # Expected GC: 0.5 (gc_fraction handles gaps)

print(f"Sequence: '{seq1}', GC Content: {data_functions.calculate_gc_content(seq1)}")
print(f"Sequence: '{seq2}', GC Content: {data_functions.calculate_gc_content(seq2)}")
print(f"Sequence: '{seq3}', GC Content: {data_functions.calculate_gc_content(seq3)}")
print(f"Sequence: '{seq4}', GC Content: {data_functions.calculate_gc_content(seq4)}")
print(f"Sequence: '{seq5}', GC Content: {data_functions.calculate_gc_content(seq5)}")
print(f"Sequence: '{seq6}', GC Content: {data_functions.calculate_gc_content(seq6)}")

# Test with invalid input
print(f"Sequence: {123}, GC Content: {data_functions.calculate_gc_content(123)}")

"""
## Testing Homopolymers
"""


# Test cases for homopolymer analysis
seq_hp1 = "AAAGGGGGTTTTTTCXXXAAAAA" # A: 3, 5; G: 5; T: 6; C: 1
seq_hp2 = "ACGTACGT" # No homopolymers >= 3
seq_hp3 = "AAAAAAAAAAAAAAAAAAAA" # A: 20
seq_hp4 = "acgtgggggaaaaa" # g: 5, a: 5 (test case insensitivity)
seq_hp5 = "" # Empty sequence

print(f"--- Testing sequence: '{seq_hp1}' ---")
print(f"min_len=5: {json.dumps(data_functions.analyze_homopolymers(seq_hp1, min_len=5), indent=2)}")
# Expected for min_len=5: {'A': [5], 'C': [], 'G': [5], 'T': [6], 'total_count': 3, 'max_len': 6}
print(f"min_len=3: {json.dumps(data_functions.analyze_homopolymers(seq_hp1, min_len=3), indent=2)}")
# Expected for min_len=3: {'A': [3, 5], 'C': [], 'G': [5], 'T': [6], 'total_count': 4, 'max_len': 6}

print(f"\n--- Testing sequence: '{seq_hp2}' ---")
print(f"min_len=3: {json.dumps(data_functions.analyze_homopolymers(seq_hp2, min_len=3), indent=2)}")
# Expected for min_len=3: {'A': [], 'C': [], 'G': [], 'T': [], 'total_count': 0, 'max_len': 0}

print(f"\n--- Testing sequence: '{seq_hp3}' ---")
print(f"min_len=10: {json.dumps(data_functions.analyze_homopolymers(seq_hp3, min_len=10), indent=2)}")
# Expected for min_len=10: {'A': [20], 'C': [], 'G': [], 'T': [], 'total_count': 1, 'max_len': 20}

print(f"\n--- Testing sequence: '{seq_hp4}' ---")
print(f"min_len=4: {json.dumps(data_functions.analyze_homopolymers(seq_hp4, min_len=4), indent=2)}")
# Expected for min_len=4: {'A': [5], 'C': [], 'G': [5], 'T': [], 'total_count': 2, 'max_len': 5}


print(f"\n--- Testing sequence: '{seq_hp5}' ---")
print(f"min_len=5: {json.dumps(data_functions.analyze_homopolymers(seq_hp5, min_len=5), indent=2)}")
# Expected for min_len=5: None

"""
## Testing Ambiguity Codes
"""

# Test cases for ambiguity analysis
seq_amb1 = "ATGCYATGR" # Y=1, R=1, total=2
seq_amb2 = "ACGTACGT" # No ambiguity
seq_amb3 = "NNNNNNNNNN" # N=10, total=10
seq_amb4 = "ATGCnATGCy" # n=1, y=1, total=2 (test case insensitivity)
seq_amb5 = "" # Empty sequence

print(f"--- Testing sequence: '{seq_amb1}' ---")
print(json.dumps(data_functions.analyze_ambiguity(seq_amb1), indent=2))
# Expected: {'total_count': 2, 'frequency': 0.25, 'counts_per_code': {'Y': 1, 'R': 1}}

print(f"\n--- Testing sequence: '{seq_amb2}' ---")
print(json.dumps(data_functions.analyze_ambiguity(seq_amb2), indent=2))
# Expected: {'total_count': 0, 'frequency': 0.0, 'counts_per_code': {}}

print(f"\n--- Testing sequence: '{seq_amb3}' ---")
print(json.dumps(data_functions.analyze_ambiguity(seq_amb3), indent=2))
# Expected: {'total_count': 10, 'frequency': 1.0, 'counts_per_code': {'N': 10}}

print(f"\n--- Testing sequence: '{seq_amb4}' ---")
print(json.dumps(data_functions.analyze_ambiguity(seq_amb4), indent=2))
# Expected: {'total_count': 2, 'frequency': 0.2, 'counts_per_code': {'N': 1, 'Y': 1}}

print(f"\n--- Testing sequence: '{seq_amb5}' ---")
print(json.dumps(data_functions.analyze_ambiguity(seq_amb5), indent=2))
# Expected: None

"""
## Consolidate Match Metrics
"""

# Assuming 'matched' is the list of matched pairs from Step 2.3 testing
# If you ran Step 2.3 testing, 'matched' should be available.
# If not, you might need to re-run that cell or create a small sample list:
# matched_mock = [{'sample_id': 'S1', 'dorado': {...}, 'guppy': {...}, 'alignment': {...}, ...}, ...] # From Step 2.3 output
try:
     # Check if 'matched' exists from previous steps
     if 'matched' in globals() and isinstance(matched, list):
          print(f"Using 'matched' list with {len(matched)} pairs.")
          input_matched_list = matched
     else:
          # Add fallback or error if 'matched' isn't available
          print("Warning: 'matched' list not found. Testing with empty list.")
          input_matched_list = []
except NameError:
     print("Warning: 'matched' list not found. Testing with empty list.")
     input_matched_list = []
run_comparison_df = data_functions.generate_comparison_dataframe(input_matched_list)

print("\nDataFrame Info:")
run_comparison_df.info()

print("\nDataFrame Head:")
display(run_comparison_df.head())

print("\nDataFrame Description (Numeric columns):")
display(run_comparison_df.describe())

"""
## Test Statistical Functions
"""

# --- Test Statistical Wrapper ---
print("--- Testing perform_paired_nonparametric_test ---")
list1 = [10, 12, 15, 11, 14, 16]
list2 = [8, 11, 13, 10, 12, 13] # Generally lower
list3 = [10, 12, 15, 11, 14, 16] # Identical to list1
list4 = [20, 22, 25, 21, 24, 26] # Generally higher

result12 = data_functions.perform_paired_nonparametric_test(list1, list2)
result13 = data_functions.perform_paired_nonparametric_test(list1, list3) # Should handle zero differences
result14 = data_functions.perform_paired_nonparametric_test(list1, list4)

print(f"Test List1 vs List2: Stat={result12[0] if result12 else 'N/A'}, p={result12[1] if result12 else 'N/A'}") # Expect potentially significant
print(f"Test List1 vs List3 (Identical): Stat={result13[0] if result13 else 'N/A'}, p={result13[1] if result13 else 'N/A'}") # Expect p=1.0 or warning
print(f"Test List1 vs List4: Stat={result14[0] if result14 else 'N/A'}, p={result14[1] if result14 else 'N/A'}") # Expect potentially significant

# Test edge cases
result_short = data_functions.perform_paired_nonparametric_test([1], [2])
result_empty = data_functions.perform_paired_nonparametric_test([], [])
result_mismatch = data_functions.perform_paired_nonparametric_test([1, 2], [3])
print(f"Test Short: {result_short}")
print(f"Test Empty: {result_empty}")
print(f"Test Mismatch Length: {result_mismatch}")

# --- Test Run-Specific Statistics ---
print("\n--- Testing calculate_run_statistics ---")

# Assume 'run_comparison_df' is available from Step 3.4 for a selected run
if 'run_comparison_df' in globals() and isinstance(run_comparison_df, pd.DataFrame) and not run_comparison_df.empty:
    run_stats_results = data_functions.calculate_run_statistics(run_comparison_df)

    if run_stats_results:
        print("Successfully calculated run statistics:")
        # Pretty print the results dictionary
        print(json.dumps(run_stats_results, indent=2, default=str)) # Use default=str for potential numpy types

        # Example: Check RiC results
        if 'RiC' in run_stats_results:
            print(f"\nRiC Median Difference: {run_stats_results['RiC'].get('median_diff')}")
            print(f"RiC p-value: {run_stats_results['RiC'].get('p_value')}")
    else:
        print("Failed to calculate run statistics.")
else:
    print("Skipping test: 'run_comparison_df' not available or empty.")
    # Create a small mock DataFrame for testing if needed
    # mock_df = pd.DataFrame({
    #      'Dorado_RiC': [10, 12, 15, 11, 14, 16, np.nan],
    #      'Guppy_RiC': [8, 11, 13, 10, 12, 13, 9],
    #      'Dorado_Length': [100, 102, 105, 101, 104, 106, 100],
    #      'Guppy_Length': [98, 101, 103, 100, 102, 103, 99],
    #      # Add other columns as needed... ensure they match metric_pairs
    # })
    # mock_results = data_functions.calculate_run_statistics(mock_df)
    # print("Mock Results:")
    # print(json.dumps(mock_results, indent=2, default=str))

"""
## Generate Summary TSVs
"""

# --- Test Run-Specific Output ---
print("\n--- Testing save_run_comparison ---")

# Assume 'run_comparison_df' is available from Step 3.4 for a selected run
# Assume 'selected_run_id' and 'RESULTS_DIR' are defined
test_run_id = 'OMDL_Test' # Use a mock ID or the selected_run_id
mock_df_for_saving = run_comparison_df # Or create a small mock DataFrame if needed

if 'mock_df_for_saving' in globals() and isinstance(mock_df_for_saving, pd.DataFrame) and not mock_df_for_saving.empty:
    # Test saving as TSV
    saved_tsv_path = data_functions.save_run_comparison(
        mock_df_for_saving,
        test_run_id,
        RESULTS_DIR,
        format='tsv'
    )
    if saved_tsv_path and os.path.exists(saved_tsv_path):
         print(f"TSV file check successful: {saved_tsv_path}")
         # Optional: Read back file to verify content
         # check_df = pd.read_csv(saved_tsv_path, sep='\t')
         # print(f"Read back {len(check_df)} rows from TSV.")

    # Test saving as CSV
    saved_csv_path = data_functions.save_run_comparison(
         mock_df_for_saving,
         test_run_id,
         RESULTS_DIR,
         format='csv'
    )
    if saved_csv_path and os.path.exists(saved_csv_path):
         print(f"CSV file check successful: {saved_csv_path}")

else:
     print("Skipping test: DataFrame for saving is not available or empty.")

# --- Test Overall Summary Output ---
print("\n--- Testing generate_overall_summary ---")

# Assume 'all_runs_analysis_results' is populated by the notebook's main loop
# It should look like: {'OMDL1': {'stats': {...}, 'counts': {...}}, 'OMDL2': {...}, ...}

# Create a mock results dictionary for testing if needed:
mock_all_runs_results = {
    'OMDL1': {
        'stats': {
            'RiC': {'median_diff': 5.0, 'p_value': 0.21, 'n_pairs': 150},
            'Length': {'median_diff': -1.0, 'p_value': 0.04, 'n_pairs': 150}
            # Add other metrics...
        },
        'counts': {'matched': 150, 'dorado_only': 5, 'guppy_only': 10}
    },
    'OMDL2': {
         'stats': {
            'RiC': {'median_diff': 10.0, 'p_value': 0.001, 'n_pairs': 200},
            'Length': {'median_diff': 0.0, 'p_value': 0.95, 'n_pairs': 200}
            # Add other metrics...
         },
         'counts': {'matched': 200, 'dorado_only': 2, 'guppy_only': 3}
    }
}

# Test saving as TSV using mock data
saved_summary_path = data_functions.generate_overall_summary(
    mock_all_runs_results, # Use mock_all_runs_results or the real all_runs_analysis_results
    RESULTS_DIR,
    format='tsv'
)

if saved_summary_path and os.path.exists(saved_summary_path):
     print(f"Overall summary file check successful: {saved_summary_path}")
     # Optional: Read back file to verify content
     # check_summary_df = pd.read_csv(saved_summary_path, sep='\t')
     # print("Overall Summary DataFrame Head:")
     # display(check_summary_df.head())
else:
     print("Failed to save overall summary file.")

"""
## Test vizualizations
"""

# Inside the function that displays run analysis (called by dropdown observer)
run_id = selected_run_id
run_data = all_runs_analysis_results.get(run_id)
if run_data and 'comparison_df' in run_data:
    run_df = run_data['comparison_df']
    if not run_df.empty:
        # Example call for RiC comparison
        fig_ric, _ = plot_comparison_with_difference(
            run_df,
            dorado_col='Dorado_RiC',
            guppy_col='Guppy_RiC',
            diff_col='RiC_Difference',
            figure_title=f'{run_id} - RiC Comparison'
        )
        plt.show(fig_ric) # Display the plot

        # Example call for Identity Distribution
        fig_identity, _ = plot_histogram(
             run_df,
             metric_col='Identity_Percent',
             title=f'{run_id} - Sequence Identity Distribution',
             xlabel='Sequence Identity (%)'
        )
        plt.show(fig_identity) # Display the plot
    else:
        print("Comparison DataFrame is empty.")
else:
    print("No comparison data available for plotting.")



================================================
File: viz_handler.py
================================================
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from IPython.display import display, Markdown, clear_output, HTML
import ipywidgets as widgets
from natsort import natsorted

import re 


def plot_metric_comparison(run_df: pd.DataFrame,
                           dorado_col: str,
                           guppy_col: str,
                           title: str = None,
                           xlabel: str = None,
                           ylabel: str = None,
                           figsize: tuple = (8, 8)) -> tuple:
    """
    Creates a scatter plot comparing a metric between Dorado and Guppy.

    Args:
        run_df: DataFrame containing the run comparison data.
        dorado_col: Column name for the Dorado metric (y-axis).
        guppy_col: Column name for the Guppy metric (x-axis).
        title: Plot title.
        xlabel: Label for x-axis.
        ylabel: Label for y-axis.
        figsize: Figure size.

    Returns:
        Tuple (matplotlib Figure, matplotlib Axes).
    """
    if run_df is None or run_df.empty:
        print(f"Warning: DataFrame is empty, cannot plot {dorado_col} vs {guppy_col}.")
        return None, None
    if dorado_col not in run_df.columns or guppy_col not in run_df.columns:
        print(f"Warning: Columns '{dorado_col}' or '{guppy_col}' not found in DataFrame.")
        return None, None

    fig, ax = plt.subplots(figsize=figsize)

    # Scatter plot
    ax.scatter(run_df[guppy_col], run_df[dorado_col], alpha=0.6, label=f'{run_df.shape[0]} pairs')

    # Default labels and title
    xl = xlabel if xlabel else guppy_col.replace('_', ' ')
    yl = ylabel if ylabel else dorado_col.replace('_', ' ')
    t = title if title else f"{yl} vs {xl}"

    ax.set_xlabel(xl)
    ax.set_ylabel(yl)
    ax.set_title(t)

    # Add diagonal line (y=x) for reference
    min_val = min(run_df[guppy_col].min(), run_df[dorado_col].min())
    max_val = max(run_df[guppy_col].max(), run_df[dorado_col].max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='y=x')

    ax.legend()
    ax.grid(True)
    plt.tight_layout()
    return fig, ax

    # Add to data_functions.py (or define in notebook)
def plot_histogram(run_df: pd.DataFrame,
                   metric_col: str,
                   title: str = None,
                   xlabel: str = None,
                   ylabel: str = 'Frequency',
                   bins: int = 30,
                   reference_line: float = None,
                   figsize: tuple = (10, 6)) -> tuple:
    """
    Creates a histogram for a specific metric.

    Args:
        run_df: DataFrame containing the run comparison data.
        metric_col: Column name for the metric to plot.
        title: Plot title.
        xlabel: Label for x-axis.
        ylabel: Label for y-axis.
        bins: Number of histogram bins.
        reference_line: Value for a vertical reference line (e.g., 0 for difference plots).
        figsize: Figure size.

    Returns:
        Tuple (matplotlib Figure, matplotlib Axes).
    """
    if run_df is None or run_df.empty:
        print(f"Warning: DataFrame is empty, cannot plot histogram for {metric_col}.")
        return None, None
    if metric_col not in run_df.columns:
        print(f"Warning: Column '{metric_col}' not found in DataFrame.")
        return None, None

    fig, ax = plt.subplots(figsize=figsize)

    # Plot histogram
    ax.hist(run_df[metric_col].dropna(), bins=bins, label=f'{run_df[metric_col].notna().sum()} values') # Drop NaN for plotting

    # Default labels and title
    xl = xlabel if xlabel else metric_col.replace('_', ' ')
    t = title if title else f"Distribution of {xl}"

    ax.set_xlabel(xl)
    ax.set_ylabel(ylabel)
    ax.set_title(t)

    # Add reference line if specified
    if reference_line is not None:
        ax.axvline(x=reference_line, color='r', linestyle='--', alpha=0.7, label=f'x={reference_line}')

    ax.legend()
    ax.grid(True, axis='y')
    plt.tight_layout()
    return fig, ax

    # Add to data_functions.py (or define in notebook)
def plot_comparison_with_difference(run_df: pd.DataFrame,
                                    dorado_col: str,
                                    guppy_col: str,
                                    diff_col: str,
                                    figure_title: str = None,
                                    figsize: tuple = (18, 7)) -> tuple:
    """
    Creates a combined figure with a scatter plot (Dorado vs Guppy)
    and a histogram of the differences.

    Args:
        run_df: DataFrame containing the run comparison data.
        dorado_col: Column name for the Dorado metric (scatter y-axis).
        guppy_col: Column name for the Guppy metric (scatter x-axis).
        diff_col: Column name for the difference metric (histogram).
        figure_title: Overall title for the combined figure.
        figsize: Figure size for the entire figure.

    Returns:
        Tuple (matplotlib Figure, array of matplotlib Axes).
    """
    if run_df is None or run_df.empty:
        print(f"Warning: DataFrame is empty, cannot create combined plot.")
        return None, None
    required_cols = [dorado_col, guppy_col, diff_col]
    if not all(col in run_df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in run_df.columns]
        print(f"Warning: Missing columns for combined plot: {missing}")
        return None, None

    fig, axes = plt.subplots(1, 2, figsize=figsize)

    # --- Scatter Plot (Left) ---
    scatter_ax = axes[0]
    scatter_ax.scatter(run_df[guppy_col], run_df[dorado_col], alpha=0.6, label=f'{run_df.shape[0]} pairs')
    xl_scatter = guppy_col.replace('_', ' ')
    yl_scatter = dorado_col.replace('_', ' ')
    scatter_ax.set_xlabel(f"Guppy {xl_scatter.split(' ')[-1]}") # Extract metric name
    scatter_ax.set_ylabel(f"Dorado {yl_scatter.split(' ')[-1]}")
    scatter_ax.set_title(f"Dorado vs Guppy: {yl_scatter.split(' ')[-1]}")

    # Diagonal line
    min_val = min(run_df[guppy_col].min(), run_df[dorado_col].min())
    max_val = max(run_df[guppy_col].max(), run_df[dorado_col].max())
    scatter_ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='y=x')
    scatter_ax.legend()
    scatter_ax.grid(True)

    # --- Histogram Plot (Right) ---
    hist_ax = axes[1]
    hist_data = run_df[diff_col].dropna()
    hist_ax.hist(hist_data, bins=30, label=f'{len(hist_data)} values')
    # Reference line at 0
    hist_ax.axvline(x=0, color='r', linestyle='--', alpha=0.7, label='x=0')

    xl_hist = diff_col.replace('_', ' ')
    hist_ax.set_xlabel(f"Difference ({xl_hist})")
    hist_ax.set_ylabel("Frequency")
    hist_ax.set_title(f"Distribution of Differences")
    hist_ax.legend()
    hist_ax.grid(True, axis='y')

    if figure_title:
        fig.suptitle(figure_title, fontsize=16)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95] if figure_title else None) # Adjust layout for suptitle
    return fig, axes

def display_run_analysis(run_id, all_results_data):
    """Retrieves data and displays statistics and plots for the selected run_id."""

    print(f"Generating analysis display for Run ID: {run_id}")

    # --- Retrieve Data ---
    run_data = all_results_data.get(run_id)
    if not run_data:
        display(Markdown(f"**Error:** No processed data found for run `{run_id}`."))
        return

    run_df = run_data.get('comparison_df')
    run_stats = run_data.get('stats') # Dictionary of calculated statistics
    run_counts = run_data.get('counts', {})

    if run_df is None or run_df.empty:
        display(Markdown(f"**Warning:** No matched pairs found or comparison DataFrame is empty for run `{run_id}`. Cannot generate detailed plots."))
        # Display basic counts if available
        display(Markdown(f"### Basic Counts for {run_id}"))
        display(Markdown(f"- Matched Pairs: {run_counts.get('matched', 'N/A')}"))
        display(Markdown(f"- Dorado-Only Sequences: {run_counts.get('dorado_only', 'N/A')}"))
        display(Markdown(f"- Guppy-Only Sequences: {run_counts.get('guppy_only', 'N/A')}"))
        return

    # --- Display Summary Statistics --- [cite: 345]
    display(Markdown(f"### Summary Statistics for {run_id} ({run_df.shape[0]} matched pairs)"))
    if run_stats:
        # Format and display key stats nicely
        stats_display = []
        for metric, values in run_stats.items():
            if 'median_diff' in values and 'p_value' in values:
                p_val_str = f"{values['p_value']:.4f}" if values['p_value'] is not None else 'N/A'
                significance = "**(Significant)**" if values['p_value'] is not None and values['p_value'] < 0.05 else ""
                stats_display.append(f"- **{metric}**: Median Diff = {values.get('median_diff', 'N/A'):.2f}, p-value = {p_val_str} {significance} (n={values.get('n_pairs', run_df.shape[0])})")
        display(Markdown("\n".join(stats_display)))
    else:
        display(Markdown("*Statistics calculation skipped or failed for this run.*"))

    # --- Generate Plots --- [cite: 346, 347, 348]
    display(Markdown("---")) # Separator

    # Plot 1: RiC Comparison [cite: 346]
    try:
        display(Markdown(f"#### Reads in Consensus (RiC) Comparison"))
        fig_ric, _ = plot_comparison_with_difference(
            run_df,
            dorado_col='Dorado_RiC',
            guppy_col='Guppy_RiC',
            diff_col='RiC_Difference',
            figure_title=f'{run_id} - RiC Comparison'
        )
        if fig_ric: plt.show(fig_ric)
    except Exception as e:
        print(f"Error plotting RiC: {e}")

    # Plot 2: Length Comparison [cite: 347]
    try:
        display(Markdown(f"#### Sequence Length Comparison"))
        fig_len, _ = plot_comparison_with_difference(
            run_df,
            dorado_col='Dorado_Length',
            guppy_col='Guppy_Length',
            diff_col='Length_Difference',
            figure_title=f'{run_id} - Length Comparison'
        )
        if fig_len: plt.show(fig_len)
    except Exception as e:
        print(f"Error plotting Length: {e}")

    # Plot 3: GC Content Comparison [cite: 347]
    try:
        # Check if GC columns exist and have data before plotting
        if 'Dorado_GC' in run_df.columns and 'Guppy_GC' in run_df.columns and run_df[['Dorado_GC', 'Guppy_GC']].notna().all(axis=1).any():
             display(Markdown(f"#### GC Content Comparison"))
             fig_gc, _ = plot_comparison_with_difference(
                 run_df,
                 dorado_col='Dorado_GC',
                 guppy_col='Guppy_GC',
                 diff_col='GC_Difference', # Assumes this column exists from Step 3.4
                 figure_title=f'{run_id} - GC Content Comparison'
             )
             if fig_gc: plt.show(fig_gc)
        else:
             display(Markdown(f"*(Skipping GC Content plot: GC columns missing or contain only NaN values)*"))
    except Exception as e:
        print(f"Error plotting GC Content: {e}")

    # Plot 4: Identity Distribution [cite: 348]
    try:
        display(Markdown(f"#### Sequence Identity Distribution"))
        fig_identity, _ = plot_histogram(
             run_df,
             metric_col='Identity_Percent',
             title=f'{run_id} - Sequence Identity Distribution',
             xlabel='Sequence Identity (%)',
             bins=25 # More bins might be useful here
        )
        if fig_identity: plt.show(fig_identity)
    except Exception as e:
        print(f"Error plotting Identity: {e}")

    # Plot 5 & 6: Homopolymer Count/MaxLen Difference (Optional) [cite: 348]
    # Example for Homo_Count difference
    try:
        if 'Dorado_Homo_Count' in run_df.columns and 'Guppy_Homo_Count' in run_df.columns:
             run_df['Homo_Count_Difference'] = run_df['Dorado_Homo_Count'] - run_df['Guppy_Homo_Count']
             if run_df['Homo_Count_Difference'].notna().any():
                 display(Markdown(f"#### Homopolymer Count Difference (Min Length 5)"))
                 fig_homo_c, _ = plot_histogram(
                      run_df,
                      metric_col='Homo_Count_Difference',
                      title=f'{run_id} - Homopolymer Count Difference Distribution',
                      xlabel='Difference in Homopolymer Runs (Dorado - Guppy)',
                      reference_line=0
                 )
                 if fig_homo_c: plt.show(fig_homo_c)
             else:
                 display(Markdown(f"*(Skipping Homopolymer Count plot: No non-NaN difference values)*"))

    except Exception as e:
        print(f"Error plotting Homopolymer Count Difference: {e}")

    show_viewer_button = widgets.Button(description="Show Interactive Alignment Viewer", button_style='success')
    viewer_output_area = widgets.Output() # Area to display the viewer widgets

    def on_show_viewer_click(button):
         with viewer_output_area:
             clear_output(wait=True)
             # Call the function to create the viewer widgets
             create_sequence_alignment_viewer(run_id, all_results_data)

    show_viewer_button.on_click(on_show_viewer_click)

    display(Markdown("---"))
    display(Markdown("#### Interactive Alignment Viewer"))
    display(show_viewer_button)
    display(viewer_output_area) # Display the area where the viewer will appear

def create_sequence_alignment_viewer(run_id, all_results_data):
    """Creates widgets to select a sample and view its alignment."""

    print(f"Initializing alignment viewer for Run ID: {run_id}")

    # --- Get Data Needed ---
    run_data = all_results_data.get(run_id)
    if not run_data:
        display(Markdown("**Error:** Run data not found."))
        return

    matched_pairs = run_data.get('matched_pairs') # Get the list of matched pairs
    run_df = run_data.get('comparison_df') # Get the comparison DataFrame

    if not matched_pairs:
         display(Markdown("**Warning:** No matched pairs found for this run. Cannot create viewer."))
         return
    if run_df is None or run_df.empty:
         display(Markdown("**Warning:** Comparison DataFrame not found or empty. Cannot populate sample list."))
         return

    # --- Create Widgets ---
    # Dropdown for Sample ID
    sample_options = natsorted(run_df['Sample_ID'].unique().tolist())
    if not sample_options:
         display(Markdown("**Warning:** No unique Sample IDs found in comparison data."))
         return

    sample_dropdown = widgets.Dropdown(
        options=sample_options,
        description='Select Sample:',
        style={'description_width': 'initial'}
    )

    # Slider for window size (optional)
    window_slider = widgets.IntSlider(
        value=100, min=50, max=200, step=10,
        description='Window Size:', style={'description_width': 'initial'}
    )

    # Button to trigger display
    view_button = widgets.Button(description="View Alignment", button_style='info')

    # Output area for the alignment HTML
    alignment_output = widgets.Output()

    # --- Button Click Handler ---
    def on_view_button_click(button):
        selected_sample_id = sample_dropdown.value
        selected_window_size = window_slider.value

        with alignment_output:
            clear_output(wait=True)
            print(f"Searching for alignment data for Sample ID: {selected_sample_id}...")

            # Find the correct matched_pair dictionary for the selected Sample_ID
            # This assumes Sample_ID is unique enough in the context of matched pairs for this run.
            # If multiple matches exist for one Sample_ID (ambiguous case), this might just pick the first.
            target_pair = None
            for pair in matched_pairs:
                # Use Sample_ID from the run_df as the key to find the pair
                # We might need a more robust way if Sample_ID isn't unique in run_df
                # or if multiple matches for a sample exist.
                # Let's assume run_df has unique Sample_IDs for now, or we take the first match.
                 if pair.get('sample_id') == selected_sample_id:
                     # Additional check: Match headers if possible to be more specific,
                     # requires headers to be present in the run_df row.
                     # row = run_df[run_df['Sample_ID'] == selected_sample_id].iloc[0]
                     # if pair['dorado']['header'] == row['Dorado_Header']: # Requires these cols in run_df
                          target_pair = pair
                          break # Found the first match for this Sample ID

            if target_pair and 'alignment' in target_pair:
                display(Markdown(f"**Displaying Alignment for Sample:** {selected_sample_id}"))
                # Display some basic info about the pair
                display(Markdown(f"- Dorado Header: `{target_pair['dorado'].get('header', 'N/A')}`"))
                display(Markdown(f"- Guppy Header: `{target_pair['guppy'].get('header', 'N/A')}`"))
                display(Markdown(f"- Identity: {target_pair['alignment'].get('identity', 'N/A'):.2f}% | Mismatches: {target_pair['alignment'].get('mismatches', 'N/A')} | Insertions: {target_pair['alignment'].get('insertions', 'N/A')} | Deletions: {target_pair['alignment'].get('deletions', 'N/A')}"))
                display(Markdown("---"))

                # Call the HTML formatting function
                html_display = format_alignment_html(target_pair, selected_window_size)
                display(html_display)
            else:
                print(f"Could not find alignment data for Sample ID: {selected_sample_id}")

    # Link button event
    view_button.on_click(on_view_button_click)

    # --- Display Widgets ---
    controls = widgets.VBox([
        widgets.HTML("Select a matched sample pair and click 'View Alignment':"), # Instructions
        sample_dropdown,
        window_slider,
        view_button
    ])
    display(widgets.VBox([controls, alignment_output]))

def format_alignment_html(alignment_dict: dict, window_size: int = 100) -> HTML:
    """
    Formats a Biopython pairwise alignment into HTML with highlighting.

    Args:
        alignment_dict: The dictionary for a matched pair, MUST contain
                        the 'alignment' sub-dictionary which includes the
                        'alignment_obj' (a Bio.Align.Alignment object).
        window_size: Number of bases to show per line chunk.

    Returns:
        IPython.display.HTML object containing the formatted alignment.
    """
    if not alignment_dict or 'alignment' not in alignment_dict or 'alignment_obj' not in alignment_dict['alignment']:
        return HTML("<p>Error: Alignment data is missing or invalid.</p>")

    alignment = alignment_dict['alignment']['alignment_obj']
    identity_percent = alignment_dict['alignment'].get('identity', 0) # Get identity from dict

    if alignment is None:
         return HTML("<p>Error: Alignment object not found in provided data.</p>")

    try:
        # Biopython's Alignment object can be indexed to get aligned sequences
        # The structure might vary slightly based on Biopython version.
        # Often it's alignment[0] for target (e.g., Dorado) and alignment[1] for query (e.g., Guppy)

        # Safely access aligned sequences
        if hasattr(alignment, 'target') and hasattr(alignment, 'query'):
             aligned_dorado = str(alignment.target) # Adapt if keys are different
             aligned_guppy = str(alignment.query)
        elif isinstance(alignment, (list, tuple)) and len(alignment) >= 2 and hasattr(alignment[0], 'seq') and hasattr(alignment[1], 'seq'):
             # Handle older or different alignment object structures if needed
             aligned_dorado = str(alignment[0].seq)
             aligned_guppy = str(alignment[1].seq)
        else:
            aligned_seqs_str = str(alignment) # Get the full string representation
            lines = aligned_seqs_str.strip().split('\n')
            aligned_dorado = ""
            aligned_guppy = ""
            # Counter to track lines within a potential alignment block (target, match, query)
            # We assume target is line 0, query is line 2 of these blocks
            line_counter_in_block = 0

            for line in lines:
                line_strip = line.strip()
                if not line_strip: continue # Skip empty lines
                # Find the sequence part (bases and dashes) at the end of the line
                seq_part_match = re.search(r'[ACGTN-]+$', line)

                if seq_part_match:
                    seq_part = seq_part_match.group(0) # Extract the sequence part
                    # Assuming Target (Dorado) is the first sequence line in a block
                    if line_counter_in_block == 0:
                        aligned_dorado += seq_part
                    # Assuming Query (Guppy) is the third sequence line in a block
                    elif line_counter_in_block == 2:
                        aligned_guppy += seq_part
                # Increment and wrap the counter (assumes blocks of 3 lines: target, match, query)
                # Only increment if we likely processed a line within a block (heuristic: it contained sequence)
                # A more robust parser might be needed if format isn't strictly 3 lines always
                if seq_part_match:
                    # This simple modulo assumes strict 3-line blocks; adjust if format varies
                    line_counter_in_block = (line_counter_in_block + 1) % 3
                else:
                    # Reset if we encounter a line without sequence (e.g., header/footer)
                    line_counter_in_block = 0
            else:
                 raise ValueError("Could not parse aligned sequences from alignment object string representation.")

        # --- Debug Prints (Keep these) ---
        print("\n" + "=" * 20 + " DEBUG Alignment Data " + "=" * 20)
        print(f"Sample ID (from dict): {alignment_dict.get('sample_id', 'N/A')}") # Make sure sample_id is passed if you want it here
        print(f"Raw Alignment Object (`alignment` variable):\n{alignment}")
        print("-" * 62)
        print(f"Extracted Dorado String (len={len(aligned_dorado)}):")
        print(aligned_dorado)
        print("-" * 62)
        print(f"Extracted Guppy String (len={len(aligned_guppy)}):")
        print(aligned_guppy)
        print("=" * 62 + "\n")
        # --- End Debug Prints ---
        seq_length = len(aligned_dorado)

        if seq_length != len(aligned_guppy):
            print(f"ERROR in format_alignment_html: Aligned sequence lengths still differ after parsing!")
            print(f"  Dorado Length: {seq_length}, Guppy Length: {len(aligned_guppy)}")
            return HTML("<p><b>Error: Cannot display alignment. Internal inconsistency detected (aligned sequence lengths differ AFTER parsing). Check parsing logic.</b></p>")

        if seq_length == 0:
             return HTML("<p>Error: Alignment resulted in empty sequences.</p>")


        # Start HTML generation
        html_parts = []
        html_parts.append(f"<h4>Sequence Alignment (Identity: {identity_percent:.2f}%)</h4>")
        html_parts.append("<div style='font-family: monospace; line-height: 1.6;'>") # Monospace font

        # Legend
        html_parts.append("<div style='margin-bottom:10px;'>")
        html_parts.append("<span style='background-color:#c8e6c9; padding: 2px 5px; border-radius: 3px; margin-right: 5px;'>Match</span>")
        html_parts.append("<span style='background-color:#f8bbd0; padding: 2px 5px; border-radius: 3px; margin-right: 5px;'>Mismatch</span>")
        html_parts.append("<span style='background-color:#bbdefb; padding: 2px 5px; border-radius: 3px;'>Gap</span>")
        html_parts.append("</div>")

        # Process alignment in chunks
        for i in range(0, seq_length, window_size):
            chunk_dorado = aligned_dorado[i:min(i + window_size, seq_length)]
            chunk_guppy = aligned_guppy[i:min(i + window_size, seq_length)]

            html_d = ""
            html_g = ""
            match_line = ""

            for j in range(len(chunk_dorado)):
                d_char = chunk_dorado[j]
                g_char = chunk_guppy[j]

                if d_char == g_char: # Match
                    style = 'background-color:#c8e6c9;'
                    match_line += "|"
                elif d_char == '-' or g_char == '-': # Gap
                    style = 'background-color:#bbdefb;'
                    match_line += " "
                else: # Mismatch
                    style = 'background-color:#f8bbd0;'
                    match_line += "."

                html_d += f"<span style='{style}'>{d_char}</span>"
                html_g += f"<span style='{style}'>{g_char}</span>"

            # Add chunk to HTML
            html_parts.append(f"<div style='margin-bottom: 15px;'>")
            html_parts.append(f"<div>Position {i+1} - {min(i+window_size, seq_length)}</div>")
            html_parts.append(f"<div>Dorado: {html_d}</div>")
            html_parts.append(f"<div style='color: #777;'>Match: &nbsp;{match_line}</div>") # Match line
            html_parts.append(f"<div>Guppy: &nbsp;{html_g}</div>") # Add space for alignment
            html_parts.append("</div>")

        html_parts.append("</div>") # Close monospace div
        return HTML(''.join(html_parts))

    except Exception as e:
        # Catch any error during formatting
        return HTML(f"<p>Error generating alignment display: {str(e)}</p>")


================================================
File: .dev.blueprint.md
================================================
## Project Blueprint: Dorado vs. Guppy Nanopore Basecaller Comparison

This blueprint outlines the steps to build the analysis tool, focusing on modularity (`data_functions.py`) and interactive exploration (`omdl_basecaller_comparison.ipynb`).

**Phase 1: Core Data Loading and Setup**

* **Goal:** Establish the project structure, basic data loading functions, and run discovery.
* **Rationale:** Foundation for all subsequent steps. Reuses basic parsing logic from the previous implementation.

    1.  [X] **Step 1.1: Project Setup & Environment**
        * **Goal:** Create the project directory structure and set up the Python environment.
        * **Module/File:** Project root, `requirements.txt`.
        * **Implementation:**
            * Create directories: `nanopore_consensus_stats/`, `seqs/`, `summary/`, `results/`.
            * Create `requirements.txt` listing core dependencies (pandas, numpy, biopython>=1.80, scipy, matplotlib, seaborn, ipywidgets, natsort, openpyxl). Consider pinning versions later.
            * Create empty `data_functions.py` and `omdl_basecaller_comparison.ipynb`.
            * Set up a virtual environment and install requirements.
        * **Output:** Correct directory structure, populated `requirements.txt`, empty core files.
        * **Testing:** Environment activates, `pip install -r requirements.txt` succeeds.

    2.  [X] **Step 1.2: Implement Run Discovery**
        * **Goal:** Create a function to find available runs and check for paired Dorado/Guppy data.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Path to the `seqs/` directory.
        * **Implementation:**
            * Define `BASE_DIR`, `SEQS_DIR`, `SUMMARY_DIR`, `RESULTS_DIR` constants **IN NOTEBOOK**
            * Implement `natsorted` module import for run sorting.
            * Implement `extract_run_info(filename)` (reuse prior logic).
            * Implement `discover_runs(seqs_dir)`:
                * Use `glob` to find FASTA files in `seqs_dir`.
                * Use `extract_run_info` to parse run ID and basecaller.
                * Store results in a dictionary `{run_id: {'dorado': bool, 'guppy': bool}}`.
                * Convert to a pandas DataFrame, naturally sort by Run ID, add 'Both Available' column.
                * Handle cases with no runs found.
                * Return the DataFrame and the dictionary.
        * **Output:** `discover_runs` function.
        * **Testing:** In the notebook, call `discover_runs` and display the resulting DataFrame. Verify it correctly identifies runs and paired data based on dummy files in `seqs/`.

    3.  [X] **Step 1.3: Implement Sequence Data Loading**
        * **Goal:** Load FASTA sequences, parsing relevant header information.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Run ID (e.g., "OMDL1"), basecaller ("dorado" or "guppy"), `SEQS_DIR` path.
        * **Implementation:**
            * Implement `extract_sample_id(header)` (reuse prior logic).
            * Implement `parse_ric_value(header)` (reuse prior logic).
            * Implement `load_sequences(run_id, basecaller)`:
                * Construct the filepath using `os.path.join`.
                * Check if file exists; return `None` if not.
                * Use `Bio.SeqIO.parse` to read the FASTA file.
                * For each record, extract `sample_id`, `ric`, `length`, and store sequence as string.
                * Store results in a `defaultdict(list)` mapping `sample_id` to a list of dictionaries, each containing `header`, `sequence`, `length`, `ric`, and potentially the `SeqRecord` object.
                * Return the dictionary.
        * **Output:** `load_sequences` function and helpers.
        * **Testing:** In the notebook, select a valid run ID and basecaller, call `load_sequences`, and inspect the structure of the returned dictionary. Check a few entries for correctness.

    4.  [X] **Step 1.4: Implement Summary Data Loading (Optional, maybe unused)**
        * **Goal:** Load summary statistics from the `.txt` files. While sequence headers contain RiC and Sample ID, summary files provide quick access to total counts.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Run ID, basecaller, `SUMMARY_DIR` path.
        * **Implementation:**
            * Implement `load_summary(run_id, basecaller)`:
                * Construct filepath.
                * Check if file exists; return `None` if not.
                * Read the main table using `pd.read_csv(filepath, sep='\t')`.
                * Read the file again line-by-line to parse the trailing summary statistics (Total Unique Samples, Total Consensus Sequences, Total RiC).
                * Return a dictionary containing the DataFrame (`'data'`) and the parsed stats (`'stats'`).
        * **Output:** `load_summary` function.
        * **Testing:** In the notebook, call `load_summary` for a valid run/basecaller and inspect the returned dictionary and DataFrame.

**Phase 2: Sequence Matching and Core Alignment**

* **Goal:** Implement the logic to match sequences between Dorado and Guppy datasets for the same sample.
* **Rationale:** This is crucial for paired comparisons. Leverages the improved matching logic discussed.

    1.  [X] **Step 2.1: Implement K-mer Similarity Function**
        * **Goal:** Create a function for fast sequence similarity estimation using k-mers.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Two sequence strings (`seq1`, `seq2`), k-mer size (`k`).
        * **Implementation:** Implement `calculate_kmer_similarity(seq1, seq2, k=7)` (reuse logic). Handle edge cases like short sequences. Ensure it returns a percentage score.
        * **Output:** `calculate_kmer_similarity` function.
        * **Testing:** Test with known similar and dissimilar sequences. Test edge cases (empty strings, short strings).

    2.  [X] **Step 2.2: Implement Pairwise Alignment Wrapper**
        * **Goal:** Create a robust wrapper around `Bio.Align.PairwiseAligner` to perform global alignment and extract key metrics.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Two sequence strings (`seq1`, `seq2`).
        * **Implementation:**
            * Implement `align_sequences(seq1, seq2)`:
                * Use `Bio.Align.PairwiseAligner` with appropriate scoring (e.g., match=1, mismatch=0, gap penalties might need tuning, but start simple as in ). Set `mode='global'`.
                * Handle potential `OverflowError` or other exceptions by possibly falling back to a basic identity calculation or returning default/error values. *Avoid* the complex approximate alignment from the old codeunless absolutely necessary and clearly documented.
                * If alignment succeeds, extract the first alignment result.
                * Format the alignment to get aligned strings.
                * Calculate Identity %, mismatches, insertions (gaps in seq2), deletions (gaps in seq1).
                * Return a dictionary: `{'identity': %, 'mismatches': N, 'insertions': N, 'deletions': N, 'alignment_obj': alignment, 'aligned_seq1': str, 'aligned_seq2': str}`.
        * **Output:** `align_sequences` function.
        * **Testing:** Test with known sequences, including identical, slightly different, and sequences with indels. Test error handling.

    3.  [X] **Step 2.3: Implement Sequence Matching Logic**
        * **Goal:** Match sequences based on Sample ID, using k-mer similarity and alignment.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Dorado sequences dict, Guppy sequences dict (outputs from Step 1.3).
        * **Implementation:** Implement `match_sequences(dorado_seqs, guppy_seqs)`:
            * Identify common Sample IDs.
            * Iterate through common Sample IDs.
            * **Case 1: 1 Dorado seq, 1 Guppy seq:** Directly align them using `align_sequences`. Add to `matched_pairs` list. Mark as `multiple_matches: False`.
            * **Case 2: Multiple sequences in either/both:**
                * Use `calculate_kmer_similarity` for all Dorado vs. Guppy pairs within the sample. Filter pairs below a threshold (e.g., 50%).
                * Perform full alignment using `align_sequences` only on pairs passing the k-mer filter. Cache alignment results to avoid re-computation.
                * Implement logic to assign matches greedily based on highest identity, ensuring one sequence is used only once *unless* multiple high-quality matches exist.
                * Flag pairs considered ambiguous (multiple plausible matches for a single sequence) with `multiple_matches: True`. Add confidence info ('high', 'medium', 'low', 'ambiguous') based on identity score.
                * Store results in `matched_pairs`.
            * Identify sequences not part of any match and add them to `dorado_only` and `guppy_only` lists.
            * Return `matched_pairs`, `dorado_only`, `guppy_only` lists.
        * **Output:** `match_sequences` function.
        * **Testing:** Test with diverse scenarios: simple 1:1 matches, samples with multiple sequences, samples only present in one dataset, samples with highly similar sequences causing ambiguity. Inspect the returned lists for correctness.

**Phase 3: Analysis Metrics Calculation**

* **Goal:** Implement functions to calculate the core and taxonomic relevance metrics.
* **Rationale:** Generates the quantitative data needed for comparison and statistics.

    1.  [X] **Step 3.1: Calculate Basic Sequence Metrics**
        * **Goal:** Ensure functions exist to calculate Length and GC Content.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Sequence string or `SeqRecord`.
        * **Implementation:**
            * Length is straightforward (`len(sequence)`).
            * GC Content: Use `Bio.SeqUtils.gc_fraction`. Implement a simple wrapper function `calculate_gc_content(sequence_str)` that handles potential non-DNA characters if necessary (though `gc_fraction` might handle this) and returns the fraction.
        * **Output:** Helper functions (or confirm direct use is sufficient).
        * **Testing:** Test `calculate_gc_content` with known sequences.

    2.  [X] **Step 3.2: Calculate Homopolymer Metrics**
        * **Goal:** Calculate statistics on homopolymer runs within sequences.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Sequence string. Minimum homopolymer length threshold (e.g., `min_len=5`).
        * **Implementation:**
            * Implement `analyze_homopolymers(sequence_str, min_len=5)`:
                * Use regex (`re` module) to find all occurrences of A{min_len,}, T{min_len,}, C{min_len,}, G{min_len,}.
                * Return a dictionary summarizing results, e.g., `{'A': [lengths], 'T': [lengths], 'C': [lengths], 'G': [lengths], 'total_count': N, 'max_len': M}`.
        * **Output:** `analyze_homopolymers` function.
        * **Testing:** Test with sequences containing various homopolymer runs.

    3.  [X] **Step 3.3: Calculate Ambiguity Metrics**
            * **Goal:** Calculate the count and frequency of all standard IUPAC ambiguity codes (R, Y, M, K, S, W, H, B, V, D, N). Primarily for future use, as current data is not expected to contain these.
            * **Module/File:** `data_functions.py`.
            * **Inputs:** Sequence string.
            * **Implementation:**
                * Implement `analyze_ambiguity(sequence_str)`:
                    * Define standard IUPAC ambiguity codes.
                    * Count occurrences of each ambiguity code (case-insensitive).
                    * Calculate total count and overall frequency (`total_count / length`).
                    * Return a dictionary `{'total_count': N, 'frequency': float, 'counts_per_code': {dict}}`, where the inner dict maps each found code to its count. Return `None` for invalid/empty input.
            * **Output:** `analyze_ambiguity` function.
            * **Testing:** Test with sequences containing various IUPAC codes (including 'N'), sequences without ambiguity, and edge cases (empty string).

    4.  [X] **Step 3.4: Consolidate Metrics for Matched Pairs**
        * **Goal:** Create a function or process to generate a DataFrame containing all relevant metrics for matched pairs, ready for statistical analysis and output.
        * **Module/File:** `data_functions.py` or processing step within the notebook.
        * **Inputs:** `matched_pairs` list (from Step 2.3).
        * **Implementation:**
            * Iterate through `matched_pairs`.
            * For each pair, retrieve/calculate:
                * Sample ID, Dorado Header, Guppy Header
                * Dorado RiC, Guppy RiC, RiC Difference
                * Dorado Length, Guppy Length, Length Difference
                * Dorado GC, Guppy GC, GC Difference (using `calculate_gc_content`)
                * Identity %, Mismatches, Insertions, Deletions (from alignment results)
                * Dorado/Guppy Homopolymer stats (using `analyze_homopolymers`)
                * Dorado/Guppy Ambiguity stats (using `analyze_ambiguity`)
                * Match quality/confidence info.
            * Compile results into a list of dictionaries.
            * Convert the list into a pandas DataFrame.
        * **Output:** A pandas DataFrame (`run_comparison_df`).
        * **Testing:** Generate the DataFrame for a sample run and verify columns and values.

**Phase 4: Statistical Analysis and Output Generation**

* **Goal:** Implement non-parametric statistical tests and generate output files.
* **Rationale:** Provides statistical rigor and shareable results.

    1.  [X] **Step 4.1: Implement Statistical Test Wrapper**
        * **Goal:** Create a function to perform non-parametric paired tests.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** Two lists/arrays of paired numerical data (e.g., Dorado RiC, Guppy RiC).
        * **Implementation:**
            * Implement `perform_paired_nonparametric_test(data1, data2, test_type='wilcoxon')`:
                * Use `scipy.stats.wilcoxon` for paired data (would need updating to use Wilcoxon instead of t-test).
                * Handle potential errors (e.g., insufficient data).
                * Return test statistic and p-value.
        * **Output:** `perform_paired_nonparametric_test` function.
        * **Testing:** Test with sample data, verify results against known examples or `scipy` documentation.

    2.  [X] **Step 4.2: Perform Run-Specific Statistical Analysis**
        * **Goal:** Apply statistical tests to the consolidated metrics DataFrame for a single run.
        * **Module/File:** `data_functions.py` or notebook cell.
        * **Inputs:** `run_comparison_df` (from Step 3.4).
        * **Implementation:**
            * Call `perform_paired_nonparametric_test` for RiC, Length, GC Content, Homopolymer counts/lengths, Ambiguity counts.
            * Calculate descriptive statistics (median difference, mean difference - use cautiously, IQR).
            * Store results in a dictionary.
        * **Output:** Dictionary containing statistical results for the run.
        * **Testing:** Run analysis on a sample DataFrame, check results for plausibility.

    3.  [X] **Step 4.3: Generate Run-Specific TSV/CSV Output**
        * **Goal:** Save the detailed comparison DataFrame for a single run.
        * **Module/File:** `data_functions.py`.
        * **Inputs:** `run_comparison_df` (Step 3.4), `run_id`, `RESULTS_DIR` path.
        * **Implementation:**
            * Implement `generate_run_output(run_df, run_id, output_dir)`:
                * Construct filename (e.g., `{run_id}_comparison_data.csv`).
                * Use `df.to_csv(filepath, sep='\t', index=False)` (or use comma for CSV). Adapt from.
        * **Output:** `generate_run_output` function. Saved TSV/CSV file.
        * **Testing:** Generate file for a sample run, inspect the file content and formatting.

    4.  [X] **Step 4.4: Generate Overall Summary Data and Output**
        * **Goal:** Aggregate key statistics across all processed runs and save to a summary file.
        * **Module/File:** `data_functions.py` or notebook processing loop.
        * **Inputs:** Results (DataFrames, stats dicts) from all processed runs. `RESULTS_DIR` path.
        * **Implementation:**
            * Loop through results for each run.
            * Extract/calculate summary info: Run ID, Sample counts, Sequence counts (total, matched, unique per basecaller), Median differences, p-values for key metrics (RiC, Length, etc.).
            * Compile into a list of dictionaries.
            * Convert to a pandas DataFrame (`overall_summary_df`).
            * Implement `generate_overall_summary_output(overall_df, output_dir)`:
                * Construct filename (e.g., `overall_comparison_summary.csv`).
                * Save DataFrame to CSV/TSV. Adapt from.
        * **Output:** `overall_summary_df` DataFrame, `generate_overall_summary_output` function, saved summary file.
        * **Testing:** Generate summary file after processing multiple dummy runs, inspect content.

**Phase 5: Jupyter Notebook Interface and Visualization**

* **Goal:** Build the interactive notebook for analysis execution, visualization, and exploration.
* **Rationale:** Provides the user interface for the analysis workflow.

    1.  [X] **Step 5.1: Notebook Setup and Workflow Orchestration**
        * **Goal:** Structure the notebook, import functions, and define the main workflow.
        * **Module/File:** `omdl_basecaller_comparison.ipynb`.
        * **Implementation:**
            * Add markdown cells for introduction, explanations (reuse from revised outline).
            * Import necessary libraries and `data_functions`.
            * Cell to define paths (`BASE_DIR`, etc.).
            * Cell to call `discover_runs` and display the runs DataFrame.
            * Implement a loop or function (`process_run` adapted from) that takes a `run_id`, calls loading functions (`load_sequences`, `load_summary`), matching (`match_sequences`), metric calculation (consolidate into `run_comparison_df` - Step 3.4), and statistical analysis (Step 4.2). Store results per run (e.g., in a dictionary `all_runs_data`).
            * Cell to trigger processing for all valid runs.
            * Cell to generate and save the overall summary output (Step 4.4).
        * **Output:** Structured notebook executing the core workflow. `all_runs_data` dictionary populated.
        * **Testing:** Run the notebook cells sequentially. Verify data is loaded, processed, and intermediate results look correct. Check for errors.

    2.  [X] **Step 5.2: Implement Interactive Run Selection**
        * **Goal:** Allow the user to select a run for detailed analysis using widgets.
        * **Module/File:** `omdl_basecaller_comparison.ipynb`.
        * **Implementation:**
            * Use `ipywidgets.Dropdown` populated with naturally sorted, valid run IDs from `all_runs_data.keys()`.
            * Define an observer function (`on_run_selected`) that updates a global variable (`selected_run_id`) and triggers the display/analysis functions for that run.
            * Display the dropdown.
        * **Output:** Interactive dropdown widget.
        * **Testing:** Select different runs, ensure the selection updates correctly.

    3.  [X] **Step 5.3: Implement Visualization Functions (Wrapper)**
        * **Goal:** Create reusable plotting functions, potentially wrapping existing ones.
        * **Module/File:** new file: `viz_handler.py`.
        * **Implementation:**
            * Adapt/reuse `create_metric_comparison_plot` (scatter)and `create_histogram_plot`. Ensure they handle potentially non-normal data appropriately (e.g., consider log scales if needed).
            * Adapt/reuse `create_combined_analysis_plots` (scatter + histogram). Ensure difference plots clearly label axes (Dorado - Guppy).
            * Consider creating specific functions for:
                * Plotting consensus count distributions (bar chart or histogram).
                * Plotting sequence identity distribution (histogram).
        * **Output:** Reusable plotting functions.
        * **Testing:** Call functions with sample data, check plot appearance and labels.

    4.  [X] **Step 5.4: Display Run-Specific Analysis and Visualizations**
        * **Goal:** In the notebook, display tables, statistics, and plots for the selected run.
        * **Module/File:** `omdl_basecaller_comparison.ipynb`, `viz_handler.py`.
        * **Inputs:** `selected_run_id`, `all_runs_data`.
        * **Implementation:**
            * Create functions (e.g., `display_run_analysis(run_id, run_data)`) that are called by the run selection observer.
            * Inside these functions:
                * Retrieve the relevant `run_comparison_df` and stats dict.
                * Display summary statistics (median differences, p-values) using Markdown.
                * Call plotting functions (Step 5.3) to show:
                    * Consensus count comparison.
                    * RiC comparison (scatter + diff histogram).
                    * Length comparison (scatter + diff histogram).
                    * GC comparison (scatter + diff histogram).
                    * Identity distribution (histogram).
                    * Homopolymer/Ambiguity comparisons (histograms or boxplots of differences).
                * Display tables of unmatched sequences or samples with multiple matches.
        * **Output:** Interactive display updating based on selected run.
        * **Testing:** Select different runs, verify that the displayed statistics and plots update correctly and match the underlying data.

    5.  [ ] **Step 5.5: Implement Interactive Alignment Viewer**
        * **Goal:** Allow users to view pairwise alignments of matched sequences interactively.
        * **Module/File:** `data_functions.py` (for `create_alignment_display`) and `omdl_basecaller_comparison.ipynb` (for widgets).
        * **Implementation:**
            * Refine `create_alignment_display(seq1, seq2, window_size)`: Ensure it uses the `aligned_seq1`, `aligned_seq2` strings from the `align_sequences` output (Step 2.2). Improve robustness and error handling. Keep the color highlighting logic.
            * In the notebook, create `create_sequence_alignment_viewer(run_df, run_data)`:
                * Use `ipywidgets.Dropdown` to select a Sample ID from the `run_comparison_df`.
                * Add button to trigger alignment display.
                * On button click, retrieve the correct Dorado/Guppy sequences from `all_runs_data` based on the selected Sample ID and headers in `run_df`. Call `create_alignment_display`.
                * Display widgets and output area.
        * **Output:** Interactive alignment viewer section in the notebook.
        * **Testing:** Select different samples, view alignments, test window size, check display for correctness and error handling.

**Phase 6: Refinement and Documentation**

* **Goal:** Finalize code, add documentation, and clean up.
* **Rationale:** Ensures usability, maintainability, and reproducibility.

    1.  [ ] **Step 6.1: Code Review and Refactoring**
        * Review all functions in `data_functions.py` and cells in the notebook for clarity, efficiency, and adherence to good practices.
        * Ensure consistency in naming and style. Refactor complex functions if needed.
    2.  [ ] **Step 6.2: Add Docstrings and Comments**
        * Add comprehensive docstrings to all functions in `data_functions.py` explaining purpose, arguments, return values, and any exceptions.
        * Add comments to complex code sections in both files.
        * Ensure Markdown cells in the notebook clearly explain each step of the analysis.
    3.  [ ] **Step 6.3: Finalize `README.md`**
        * Update `README.md` to accurately reflect the project's purpose, usage (how to install requirements, run the notebook), structure, and outputs.
    4.  [ ] **Step 6.4: Testing Across Multiple Runs**
        * Run the entire notebook analysis using the real dataset (multiple OMDL runs) to ensure robustness and check performance. Debug any issues arising from larger/diverse data.



#### Prompt for my own use (LLM can disregard as it will be passed in the opening prompt)
I am continuing work on a project to compare Nanopore basecallers (Dorado vs. Guppy) by re-implementing an analysis pipeline.
- Reference Implementation: reference.txt (provided for context, but improvements are desired).
- Current Work-in-Progress: digest.txt (contains the latest code state from our previous session).
- Project Context & Plan: The necessary background is in .dev.outline.md and the detailed implementation plan/progress is tracked in .dev.blueprint.md (both files are within digest.txt).

Instructions:
- Resume Point: We will pick up with Step 5.1 in Phase 5 as defined in .dev.blueprint.md. Please locate this step.
- Walkthrough: Provide a detailed, step-by-step walkthrough for implementing this single step, referencing the existing code in digest.txt and relevant concepts from reference.txt or .dev.outline.md as needed.
- Approach: Evaluate optimal methods, retain useful parts of the previous codebase (reference.txt), but prioritize new/improved implementations. Please always think critically about the best approaches for execution. Wait for my confirmation before proceeding to the next step.


================================================
File: .dev.outline.md
================================================
## Overview
This project is used to compare the sequencing data results of the same source nanopore data processed through either Dorado or Guppy basecaller; providing insights into the differences and similarities between the two datasets. Both basecallers started with the same raw source data for each respective dataset, and therefore the end results are compared to assess the expected improvements of the Dorado basecaller. All necessary data for comparison is contained in the respective fasta DNA sequence files (in `./seqs/`) and run summary txt files (in `./summary/`). The naming convention for the files includes a unique identifier for each dataset (`OMDL{number}`, AKA the "run"), followed by the basecaller used to produce the data (`_dorado` or `_guppy`). The analysis goals are open ended to start. The main objective is to compare performance of the different data pipelines used to produce each run's data set, but the anticipated differences are not fully known. 

For background context, here is an explanation of how these datasets were produced. Use this context to consider the most appropriate way to statistically compare the data.
1. These data sets are fungal ITS sequences generated from hundreds of samples that were multiplexed in individual DNA barcoding runs (`OMDL{number}`) on a MinION nanopore sequencer (long reads, expected to be 500-700nt, but is variable depending on species). The raw signal data from the nanopore sequencer is first "basecalled" to nucleotides by either the older software, Guppy, or the new software, Dorado. Each base gets a confidence score (Q-score) quantitatively measuring the expected error rate at that position. This is the first major difference in datasets. The SAME raw signal data can be re-processed with new basecaller updates to potentially gain much better quality without any lab re-work. This is the premise of this analysis project.
2. The bulk basecalled read data for each run is then "demultiplexed" in software based on finding dual-end index tags that offer unique tag combinations per sample. Demultiplexing sorts the bulk raw reads into respective sample buckets based on the matching index tag combination found on either end of the raw read. Each resulting sample bucket undergoes sequence clustering by k-mer similarity, then fine alignment to produce final consensus sequence(s) for a given specimen in the run. The number of aligned sequences used to produce a consensus is refered to as "Reads in Consensus" (AKA, "RiC" or more commonly sequencing "depth" or "coverage"). Higher RiC consensus sequences tend to produce better results by correcting per-position base errors, etc. 
3. These demultiplexing, clustering, and consensus algorithms are not perfect and the starting data is also far from ideal. There's often many erroneous raw reads included in the dataset that must be filtered out during data processing; most notably chimera sequences, and other mis-primed amplicons like primer-dimers. If the clustering and consensus algorithms mistakenly include a chimeric read, it can significantly skew the final consensus sequence. A bad index tag match could mistakenly demultiplex a read into the wrong sample bucket (AKA "cross-talk"). These factors should be considered when designing the statistical analysis.
4. Note that the specimens are not pure isolates. Therefore, latent contamination is expected and off-target ITS sequences may be produced (eg, yeast spores contaminating the mushroom tissue that was sequenced). It's also possible there may be two desired target sequences for a single specimen (eg, a parasitic fungi on a mushroom, like the common Lobster mushroom). Therefore most cases should ideally produce a single target consensus sequence with a high RiC (indicating clean, high quality/confidence results), but there are edge cases to be considered. There is also known to be intra-specific ITS gene variation in fungi. That is, a single fungal genome has MANY copies of the ITS gene, and the copies across the whole genome may have slightly variations (SNPs, indels, etc. AKA "haplotypes"). There's also slight variation between ITS sequences of the same species to be expected; this is a crucial point when considering taxonomic classification of a given ITS sequence.
5. Generally speaking, the best results have close to 1:1 ratio of consensus sequences per sample, high RiC, and accurate consensus nucleotide calls across the sequence. The latter could be empirically determined by aligning the consensus sequence to a known reference sequence, but reference sequences are not available for all samples. Therefore, we will be comparing the paired data sets using whatever metrics we can confidently choose for statistical analysis. 

This project has been implemented as an interactive Jupyter notebook (`analysis_interface.ipynb`) that leverages the functionality in the `data_functions.py` modules to provide interactive data exploration, visualization, and analysis. Special consideration should be given to modularity of the overall codebase, with the Jupyter notebook being the primary user interface, and the `data_functions` doing most (if not all) of the backend work.

### Background Context Summary
1.  **Data Source:** Multiplexed fungal ITS amplicon sequences from MinION runs.
2.  **Basecalling:** Raw signal data processed by both Guppy and Dorado.
3.  **Processing:** Demultiplexing, clustering, and consensus sequence generation lead to FASTA files (`./seqs/`) and summary tables (`./summary/`). "Reads in Consensus" (RiC) indicates sequence depth.
4.  **Challenges:** Potential issues include chimeras, cross-talk, contamination, multiple valid biological sequences (haplotypes, co-infections), and intra-specific variation.
5.  **Ideal Outcome:** Generally 1:1 consensus per sample, high RiC, accurate sequence. Direct accuracy check via references is deferred for now.

## Directory Structure
The data is organized into several directories: `seqs` for the sequence files, `summary` for the summary files, and `results` for output data generated by this program. The sequence files are in FASTA format and contain all consensus sequences generated for samples in that run. The corresponding summary files are in a tab-separated values (TSV) format but labeled as text files. These summary files contain a simple table of the list of consensus sequences and their respective length, coverage (aka "Reads in Consensus"), 
```plaintext
.
â”œâ”€â”€ data_functions.py
â”œâ”€â”€ analysis_interface.ipynb
â”œâ”€â”€ outline.md (this file)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results/
â”‚   â””â”€â”€ ... (analysis output files)
â”œâ”€â”€ seqs/
â”‚   â”œâ”€â”€ OMDL*_seqs_dorado.fasta
â”‚   â””â”€â”€ OMDL*_seqs_guppy.fasta ...
â””â”€â”€ summary/
    â”œâ”€â”€ OMDL*_summary_dorado.txt
    â””â”€â”€ OMDL*_summary_guppy.txt ...
```

### File Descriptions
- `data_functions.py`: Python module containing functions for loading, analyzing, and comparing sequence data.
- `analysis_interface.ipynb`: Jupyter notebook for interactive analysis and visualization of the comparison results.
- `outline.md`: This file, which outlines the project structure and methodology.
- `results/`: Directory where the output files (TSV data) are saved after running the analysis.
- `seqs/`: Directory containing sequence files in FASTA format.
- `summary/`: Directory containing summary files for each sequence dataset.

## Data Format Specifications
### Sequence Files
- `seqs/`: Directory containing sequence files in FASTA format.
- Filenames:`OMDL{number}_seqs_{source}.fasta`: FASTA files containing all sequences for the run.
    - `OMDL{number}`: Unique identifier for the run's dataset.
    - `{source}`: Basecaller source of the sequence data ('dorado' or 'guppy').
- The `seqs` files provide all the sequence data for the run, including info in the the sequence headers, formatted as follows:
    - `>ONT{plate_num}.{well_position}-{sampleID}-iNat{iNat_num}-{replicate_number} ric={ric_value}`
        - `ONT{plate_num}`: 96-well plate number within a run (e.g., 01), up to 10 plates per run.
        - `{well_position}`: 96-well plate position (e.g., A02), relative to each plate, repeated for different samples on up to 10 plates per run (not entirely unique).
        - `{sampleID}`: Unique sample ID (e.g., OMDL00009).
        - `iNat{iNat_num}`: Sample ID (e.g., iNat169115711). (iNat = iNaturalist; unique identifier for the sample, may be repeated if a sample is sequenced multiple times in a run).
        - `{replicate_number}`: Replicate number (e.g., 1). If multiple consensus sequences are generated from the same sample, they will be numbered sequentially (e.g., 1, 2, 3...).
        - `ric={ric_value}`: "reads in consensus" value. AKA sequencing depth/coverage (e.g., 388).
- Example contents:
```plaintext
>ONT01.09-A02-OMDL00009-iNat169115711-1 ric=388
GTAAAAGTCGTAACAAGGTTTCCGTAGGTGAAC...CTCAAATCAGGTAGGATTACCCGCTGAACTTAAGATAA
>ONT01.10-B02-OMDL00010-iNat167175587-1 ric=113
TGAAAAGTCGTAA...CTCAAATCAGGTAGGATTACCCGCTGAACTTAAGA
[remaining sequence records in fasta format...]
```

### Summary Files
**NOTE**: It was determined that these summary files are redundant and not strictly necessary. All information is either included in the sequences (name, length, RiC, multiple #), or can easily be calculated (total seqquences, total consensus seqs, total reads)
- `summary/`: Directory containing summary files for each run's dataset.
- Filenames: `OMDL{number}_summary_{source}.txt`: TSV (labeled as txt) files containing brief metrics for all sequences in run's dataset.
    - `OMDL{number}`: Unique identifier for the run's dataset.
    - `{source}`: Basecaller source of the summary data ('dorado' or 'guppy').
- The summary metrics include:
    - (column 1) `Filename`: Name of the sequence file.
    - (column 2) `Length`: Length of the sequence.
    - (column 3) `Reads in Consensus`: Number of reads in the consensus sequence (i.e., "RiC" or depth/coverage).
    - (column 4) `Multiple`: Indicates {replicate_num} of the sequence (1 = first, 2 = second, etc.) if multiple consensus sequences were generated from the same sample.
    - Appended lines at end of file:
        - `Total Unique Samples`: Total number of unique samples in the dataset.
        - `Total Consensus Sequences`: Total number of consensus sequences in the dataset.
        - `Total Reads in Consensus Sequences`: Total number of reads in all consensus sequences in the dataset.
- Example contents:
```plaintext
Filename	Length	Reads in Consensus	Multiple
ONT01.09-A02-OMDL00009-iNat169115711-1	665	388	1
ONT01.10-B02-OMDL00010-iNat167175587-1	662	113	1
[complete list of sequences...]
ONT05.95-G12-OMDL00479-iNat164356544-1	626	34	1

Total Unique Samples	152
Total Consensus Sequences	155
Total Reads in Consensus Sequences	20717
```
## Interactive Notebook Structure
The Jupyter notebook provides an interactive environment for comparative analysis:

1.  **Setup and Data Loading:**
    * Import libraries (pandas, numpy, Biopython, scipy.stats, plotting libraries, ipywidgets, natsort).
    * Define constants and paths using `data_functions.py`.
    * Discover runs with paired Dorado/Guppy data using `data_functions.discover_runs`.
    * Load sequence and summary data for valid runs using `data_functions.load_sequences` and `data_functions.load_summary`.
2.  **Sequence Matching and Initial Comparison:**
    * Match sequences between Dorado and Guppy datasets for each sample using `data_functions.match_sequences`. This handles one-to-one, one-to-many, and many-to-many scenarios.
    * Generate run-specific and overall summary dataframes containing key metrics for matched and unmatched sequences.
    * Display overall summary statistics comparing Dorado and Guppy across all runs (e.g., number of samples, sequences, matches, unique sequences).
3.  **Run-Specific Detailed Analysis (Interactive):**
    * Use `ipywidgets` for selecting a specific run (`OMDL{number}`).
    * **Consensus Sequence Count Analysis:** Compare the *number* of consensus sequences generated per sample between Dorado and Guppy. Visualize distributions and test for significant differences (e.g., using Wilcoxon signed-rank test on counts per sample).
    * **Matched Pair Analysis:**
        * **RiC Comparison:** Visualize Dorado vs. Guppy RiC for matched pairs (scatter plot, histogram of differences). Perform non-parametric paired tests (e.g., Wilcoxon signed-rank test) on RiC values.
        * **Sequence Length Comparison:** Similar visualization and non-parametric testing for sequence lengths.
        * **GC Content Comparison:** Similar visualization and non-parametric testing for GC content.
        * **Sequence Identity Analysis:** Analyze distribution of percentage identity for matched pairs. Examine mismatches, insertions, deletions. Interactively explore alignments using `data_functions.create_alignment_display`.
    * **Unmatched Sequence Analysis:** Characterize sequences unique to either Dorado or Guppy for each sample (e.g., their RiC, length distribution).
    * **Multiple Match Analysis:** Interactively explore samples where the matching algorithm found multiple potential pairings, showing sequence characteristics and similarity matrices.
    * **Taxonomic Relevance Metric Comparison:**
        * **Homopolymer Analysis:** Compare lengths of homopolymer runs (e.g., >=5 bases) in matched sequences between Dorado and Guppy.
        * **Ambiguity Analysis:** Compare the count/frequency of ambiguous bases ('N', 'R', 'Y', etc) in matched sequences.
4.  **Comprehensive Analysis and Interpretation:**
    * Provide summary interpretations based on the statistical tests and visualizations across all metrics for the selected run.
    * Offer functionality to aggregate key findings across all processed runs.
5.  **Export and Reporting:**
    * Export detailed run-specific and overall comparison data to TSV/CSV (and potentially Excel) formats. Ensure exported tables include relevant metrics (e.g., number of consensus per sample).
    * Allow exporting generated plots (PNG, SVG).
    * (Lower Priority) Note on PDF reporting: Advise users on manual export via Jupyter or using `nbconvert` via command line, rather than implementing complex automated PDF generation within the notebook.

### Key Analysis Metrics
The analysis focuses on the following key metrics to assess differences between Dorado and Guppy basecallers:
1.  **Consensus Sequence Count per Sample:** Distribution and comparison of the number of sequences generated per sample (Dorado vs. Guppy).
2.  **Matched Sequence Characteristics (Paired Comparison):**
    * RiC (Reads in Consensus): Depth of coverage.
    * Sequence Length.
    * GC Content.
    * Sequence Identity (%): Including mismatches, insertions, deletions from pairwise alignments.
3.  **Sequence Quality/Taxonomic Relevance Metrics:**
    * Homopolymer run lengths.
    * Frequency of ambiguous bases ('N', 'R', 'Y', etc).
4.  **Unmatched Sequences:** Characterization of sequences found only in Dorado or Guppy datasets for a given sample.

### Statistical Methods

* Utilize non-parametric tests appropriate for paired data where normality cannot be assumed (e.g., **Wilcoxon signed-rank test**) for comparing RiC, length, GC content, homopolymer counts, and ambiguity counts between matched Dorado and Guppy sequences.
* Employ methods to compare distributions where relevant (e.g., histograms, potentially Kolmogorov-Smirnov test for sequence counts per sample).
* Significance level (alpha) typically set at 0.05.

## Output Files
The analysis produces the following output files in the `results` directory:


1.  **Run-Specific TSV/CSV (`OMDL{number}_comparison_data.tsv`):**
    * Includes columns for Sample\_ID, Dorado/Guppy Headers, RiC, Length, GC Content (calculated via Biopython), differences, and alignment metrics (Identity %, Mismatches, Insertions, Deletions) for *matched pairs*.
    * Potentially add columns for homopolymer stats and ambiguity counts per sequence.
2.  **Overall Summary TSV/CSV (`overall_comparison_summary.tsv`):**
    * Includes Run\_ID, counts of unique samples, total consensus sequences (Dorado/Guppy), matched sequences, Dorado-only sequences, Guppy-only sequences.
    * Aggregate statistics (e.g., median differences, p-values from non-parametric tests) for RiC, Length, GC Content across runs.
    * Aggregate statistics on the number of consensus sequences per sample.
3.  **Visualizations:** Saved plots (e.g., PNG) from the notebook analysis.

### Dependencies

* Python 3.x
* **Biopython:** Core dependency for sequence parsing, alignment, GC calculation, etc..
* **Pandas:** Data manipulation and table generation.
* **NumPy:** Numerical operations.
* **SciPy (scipy.stats):** Statistical testing (non-parametric tests).
* **Matplotlib & Seaborn / Plotly:** Data visualization.
* **ipywidgets & IPython:** Interactive notebook elements.
* **natsort:** Natural sorting of run IDs.
* (Optional for Excel export): `openpyxl` or `xlsxwriter`.
* (Optional, if MSA is retained): External tool like MUSCLE, accessible in system PATH.





