{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanopore Consensus Sequence Analysis: Old Guppy Data vs New Dorado Data\n",
    "## Project Goal\n",
    "This notebook facilitates a comparative analysis of consensus sequences generated from the same raw Nanopore sequencing data (fungal ITS amplicons) but processed using two different software pipelines; most notably: **Dorado** (the newer method) and **Guppy** (the previous standard).The objective is to quantitatively assess the differences and potential improvements offered by the new Dorado pipeline by comparing key sequence metrics and characteristics.\n",
    "\n",
    "## Background\n",
    "The raw Nanopore signal data from fungal ITS sequencing runs (`OMDL*` datasets in our case) was independently processed by both basecallers in multiplex sampple pool. Subsequent steps involved demultiplexing reads into sample-specific bins, clustering similar reads, and generating consensus sequences. The final output for comparison includes FASTA sequence files and associated metadata like \"Reads in Consensus\" (RiC).  There have been many software improvements implemented since the original Guppy datasets were produced, so this comparison is to investigate the difference produced using the new pipeline on the exact same raw source data. \n",
    "\n",
    "## Notebook Workflow\n",
    "This jupyter notebook provides an interactive interface to:\n",
    "1.  **Setup and Config:** Load run data, and identify runs with paired Dorado and Guppy data, then and load the corresponding sequence files.\n",
    "2.  **Match Sequences:** Use some logic to pair corresponding consensus sequences generated by Dorado and Guppy for the *same* original sample.\n",
    "3.  **Calculate & Compare Metrics:** For matched pairs, calculate and compare key metrics like:\n",
    "    * Reads in Consensus (RiC)\n",
    "    * Sequence Length\n",
    "    * GC Content\n",
    "    * Sequence Identity (including mismatches, insertions, deletions)\n",
    "    * Homopolymer run characteristics\n",
    "    * Frequency of ambiguous bases (not currently relevant)\n",
    "4.  **Statistical Analysis:** Apply non-parametric tests (e.g., Wilcoxon signed-rank test) to assess the significance of observed differences.\n",
    "5.  **Visualize Results:** Generate plots (scatter plots, histograms) to visualize comparisons and distributions.\n",
    "6.  **Explore Alignments:** Interactively view pairwise alignments of matched sequences to examine differences at the base level.\n",
    "7.  **Summarize & Export:** Generate summary tables for individual runs and across all runs, exporting results to TSV/CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_functions\n",
    "from viz_handler import display_run_analysis, create_sequence_alignment_viewer\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import json\n",
    "# Optional: Reload data_functions if making changes during development\n",
    "# import importlib\n",
    "# importlib.reload(data_functions) # Use this in code cell to reload the module\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configure plotting style (optional)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Configuration: Define Project Paths ---\n",
    "# You can change BASE_PROJECT_DIR if your data/results aren't relative to the notebook\n",
    "BASE_PROJECT_DIR = '.' # Assumes seqs, summary, results are subdirs of the notebook's dir or a linked dir\n",
    "\n",
    "# Define specific directories relative to the base\n",
    "SEQS_DIR = os.path.join(BASE_PROJECT_DIR, 'seqs')\n",
    "SUMMARY_DIR = os.path.join(BASE_PROJECT_DIR, 'summary')\n",
    "RESULTS_DIR = os.path.join(BASE_PROJECT_DIR, 'results')\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using Sequences Directory: {os.path.abspath(SEQS_DIR)}\")\n",
    "print(f\"Using Summary Directory:   {os.path.abspath(SUMMARY_DIR)}\")\n",
    "print(f\"Using Results Directory:   {os.path.abspath(RESULTS_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df, runs_dict = data_functions.discover_runs(SEQS_DIR)\n",
    "\n",
    "print(\"Discovered Runs:\")\n",
    "display(runs_df)\n",
    "if runs_dict:\n",
    "    print(\"Run dictionary Loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Runs\n",
    "Execute the workflow on all runs loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_run_data(run_id, seqs_dir, results_dir):\n",
    "    \"\"\"Loads, matches, analyzes, and saves data for a single run.\"\"\"\n",
    "    print(f\"\\n--- Processing Run: {run_id} ---\")\n",
    "    run_results = {'run_id': run_id, 'stats': {}, 'counts': {}}\n",
    "\n",
    "    # 1. Load Sequences\n",
    "    print(\"  Loading sequences...\")\n",
    "    dorado_seqs = data_functions.load_sequences(run_id, 'dorado', seqs_dir)\n",
    "    guppy_seqs = data_functions.load_sequences(run_id, 'guppy', seqs_dir)\n",
    "\n",
    "    if dorado_seqs is None or guppy_seqs is None:\n",
    "        print(f\"  Skipping {run_id}: Missing Dorado or Guppy sequence file.\")\n",
    "        return None # Indicate failure for this run\n",
    "\n",
    "    print(f\"  Loaded {sum(len(v) for v in dorado_seqs.values())} Dorado sequences across {len(dorado_seqs)} samples.\")\n",
    "    print(f\"  Loaded {sum(len(v) for v in guppy_seqs.values())} Guppy sequences across {len(guppy_seqs)} samples.\")\n",
    "    run_results['counts']['dorado_total'] = sum(len(v) for v in dorado_seqs.values()) # Store total counts\n",
    "    run_results['counts']['guppy_total'] = sum(len(v) for v in guppy_seqs.values())\n",
    "\n",
    "    # 2. Match Sequences\n",
    "    print(\"  Matching sequences...\")\n",
    "    matched_pairs, dorado_only, guppy_only = data_functions.match_sequences(dorado_seqs, guppy_seqs)\n",
    "    run_results['matched_pairs'] = matched_pairs # Keep matched_pairs for potential later use (e.g., alignment viewer)\n",
    "    run_results['dorado_only'] = dorado_only\n",
    "    run_results['guppy_only'] = guppy_only\n",
    "    run_results['counts']['matched'] = len(matched_pairs)\n",
    "    run_results['counts']['dorado_only'] = len(dorado_only)\n",
    "    run_results['counts']['guppy_only'] = len(guppy_only)\n",
    "    print(f\"  Matching complete: {len(matched_pairs)} pairs, {len(dorado_only)} Dorado-only, {len(guppy_only)} Guppy-only.\")\n",
    "\n",
    "    if not matched_pairs:\n",
    "        print(f\"  Skipping analysis for {run_id}: No matched pairs found.\")\n",
    "        return run_results # Return counts even if no matches\n",
    "\n",
    "    # 3. Consolidate Metrics into DataFrame (Step 3.4)\n",
    "    print(\"  Generating comparison DataFrame...\")\n",
    "    run_comparison_df = data_functions.generate_comparison_dataframe(matched_pairs)\n",
    "    run_results['comparison_df'] = run_comparison_df # Store DF for this run\n",
    "\n",
    "    if run_comparison_df.empty:\n",
    "         print(f\"  Skipping further analysis for {run_id}: Comparison DataFrame is empty.\")\n",
    "         return run_results\n",
    "\n",
    "    # 4. Perform Run-Specific Statistical Analysis (Step 4.2)\n",
    "    print(\"  Calculating run statistics...\")\n",
    "    run_stats = data_functions.calculate_run_statistics(run_comparison_df)\n",
    "    run_results['stats'] = run_stats\n",
    "    print(f\"  Statistics calculated: {list(run_stats.keys()) if run_stats else 'None'}\")\n",
    "\n",
    "\n",
    "    # 5. Save Run-Specific Output (Step 4.3)\n",
    "    print(\"  Saving run comparison data...\")\n",
    "    saved_path = data_functions.save_run_comparison(run_comparison_df, run_id, results_dir, format='tsv') # Or 'csv'\n",
    "    if saved_path:\n",
    "         print(f\"  Run comparison saved to: {saved_path}\")\n",
    "         # Optionally save as CSV too\n",
    "         # data_functions.save_run_comparison(run_comparison_df, run_id, results_dir, format='csv')\n",
    "    else:\n",
    "         print(\"  Failed to save run comparison data.\")\n",
    "\n",
    "\n",
    "    print(f\"--- Finished processing {run_id} ---\")\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process All Valid Runs ---\n",
    "all_runs_analysis_results = {}\n",
    "valid_run_ids = runs_df[runs_df['Both Available']].index.tolist() # Get naturally sorted IDs from the DataFrame\n",
    "\n",
    "if not valid_run_ids:\n",
    "    print(\"No runs found with both Dorado and Guppy data available.\")\n",
    "else:\n",
    "    print(f\"Starting processing for {len(valid_run_ids)} runs: {', '.join(valid_run_ids)}\")\n",
    "    for run_id in valid_run_ids:\n",
    "         # Pass necessary directory paths to the function\n",
    "        results = process_run_data(run_id, SEQS_DIR, RESULTS_DIR)\n",
    "        if results: # Store results even if some steps failed (e.g., no matches)\n",
    "             all_runs_analysis_results[run_id] = results\n",
    "\n",
    "    print(\"\\n--- All Run Processing Complete ---\")\n",
    "    print(f\"Processed results stored in 'all_runs_analysis_results' dictionary for {len(all_runs_analysis_results)} runs.\")\n",
    "\n",
    "    # Optional: Display a summary of what was processed\n",
    "    processed_summary = []\n",
    "    for run_id, data in all_runs_analysis_results.items():\n",
    "         counts = data.get('counts', {})\n",
    "         processed_summary.append({\n",
    "              'Run_ID': run_id,\n",
    "              'Matched': counts.get('matched', 0),\n",
    "              'Dorado_Only': counts.get('dorado_only', 0),\n",
    "              'Guppy_Only': counts.get('guppy_only', 0),\n",
    "              'Stats_Keys': list(data.get('stats', {}).keys()) if data.get('stats') else 'N/A'\n",
    "         })\n",
    "    summary_df = pd.DataFrame(processed_summary)\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate and Save Overall Summary ---\n",
    "print(\"\\n--- Generating Overall Summary ---\")\n",
    "if all_runs_analysis_results:\n",
    "    overall_summary_path = data_functions.generate_overall_summary(\n",
    "        all_runs_analysis_results,\n",
    "        RESULTS_DIR,\n",
    "        format='tsv' # Or 'csv'\n",
    "    )\n",
    "    if overall_summary_path:\n",
    "        print(f\"Overall summary saved to: {overall_summary_path}\")\n",
    "        # Optionally load and display the summary DataFrame\n",
    "        try:\n",
    "            overall_summary_df = pd.read_csv(overall_summary_path, sep='\\t')\n",
    "            print(\"\\nOverall Summary DataFrame Head:\")\n",
    "            display(overall_summary_df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read back overall summary file: {e}\")\n",
    "    else:\n",
    "        print(\"Failed to generate overall summary file.\")\n",
    "else:\n",
    "    print(\"Skipping overall summary generation: No run results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Run Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Interactive Cell ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from natsort import natsorted\n",
    "from viz_handler import display_run_analysis, create_sequence_alignment_viewer # Assuming you move viewer creation too\n",
    "import data_functions # Make sure it's imported\n",
    "\n",
    "# --- 1. Define Widgets Globally ---\n",
    "# Dropdown for run selection\n",
    "processed_run_ids = list(all_runs_analysis_results.keys())\n",
    "sorted_run_ids = natsorted(processed_run_ids)\n",
    "selected_run_id = sorted_run_ids[0] if sorted_run_ids else None\n",
    "run_dropdown = widgets.Dropdown(\n",
    "    options=sorted_run_ids,\n",
    "    description='Select Run:',\n",
    "    disabled=not sorted_run_ids,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# Main output area for stats/plots\n",
    "analysis_output_area = widgets.Output()\n",
    "# Button and output area for the alignment viewer\n",
    "show_viewer_button = widgets.Button(description=\"Show Interactive Alignment Viewer\", button_style='success')\n",
    "viewer_output_area = widgets.Output() # Area for the alignment viewer widgets\n",
    "\n",
    "# --- 2. Define Event Handlers Globally ---\n",
    "# Handler for run dropdown change\n",
    "def on_run_select_change(change):\n",
    "    global selected_run_id\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        selected_run_id = change['new']\n",
    "        with analysis_output_area:\n",
    "            clear_output(wait=True)\n",
    "            # Call function to display stats/plots for the run\n",
    "            display_run_analysis(selected_run_id, all_runs_analysis_results)\n",
    "        # Clear the separate alignment viewer area when the run changes\n",
    "        with viewer_output_area:\n",
    "            clear_output()\n",
    "\n",
    "# Handler for the \"Show Alignment Viewer\" button click\n",
    "def on_show_viewer_click(button):\n",
    "     with viewer_output_area:\n",
    "         clear_output(wait=True)\n",
    "         if selected_run_id:\n",
    "             # Call function to create and display the viewer widgets\n",
    "             create_sequence_alignment_viewer(selected_run_id, all_runs_analysis_results)\n",
    "         else:\n",
    "             print(\"Please select a run first.\")\n",
    "\n",
    "# --- 3. Link Handlers ---\n",
    "run_dropdown.observe(on_run_select_change, names='value')\n",
    "show_viewer_button.on_click(on_show_viewer_click)\n",
    "\n",
    "# --- 4. Display Layout ---\n",
    "display(Markdown(\"### Select Run for Detailed Analysis:\"))\n",
    "display(run_dropdown)\n",
    "display(analysis_output_area) # Area for stats/plots\n",
    "\n",
    "display(Markdown(\"---\")) # Separator\n",
    "display(Markdown(\"#### Interactive Alignment Viewer\"))\n",
    "display(show_viewer_button) # Display button\n",
    "display(viewer_output_area) # Display area for viewer widgets\n",
    "\n",
    "# --- 5. Initial Trigger ---\n",
    "if selected_run_id:\n",
    "    on_run_select_change({'type': 'change', 'name': 'value', 'new': selected_run_id})\n",
    "else:\n",
    "    print(\"No initial run selected or no runs processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
