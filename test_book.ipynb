{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_functions\n",
    "from viz_handler import plot_metric_comparison, plot_histogram, plot_comparison_with_difference\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from IPython.display import display, Markdown # type: ignore\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import json\n",
    "# Optional: Reload data_functions if making changes during development\n",
    "# import importlib\n",
    "# importlib.reload(data_functions) # Use this in code cell to reload the module\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configure plotting style (optional)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Configuration: Define Project Paths ---\n",
    "# You can change BASE_PROJECT_DIR if your data/results aren't relative to the notebook\n",
    "BASE_PROJECT_DIR = '.' # Assumes seqs, summary, results are subdirs of the notebook's dir or a linked dir\n",
    "\n",
    "# Define specific directories relative to the base\n",
    "SEQS_DIR = os.path.join(BASE_PROJECT_DIR, 'seqs')\n",
    "SUMMARY_DIR = os.path.join(BASE_PROJECT_DIR, 'summary')\n",
    "RESULTS_DIR = os.path.join(BASE_PROJECT_DIR, 'results')\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using Sequences Directory: {os.path.abspath(SEQS_DIR)}\")\n",
    "print(f\"Using Summary Directory:   {os.path.abspath(SUMMARY_DIR)}\")\n",
    "print(f\"Using Results Directory:   {os.path.abspath(RESULTS_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_id = 'OMDL1' # Replace with a valid run ID from your data\n",
    "test_basecaller = 'dorado' # Replace 'dorado' or 'guppy' as available\n",
    "\n",
    "loaded_sequences = data_functions.load_sequences(test_run_id, test_basecaller, SEQS_DIR)\n",
    "\n",
    "if loaded_sequences is not None:\n",
    "    print(f\"Successfully loaded data for {test_run_id} {test_basecaller}.\")\n",
    "    print(f\"Found data for {len(loaded_sequences)} unique sample IDs.\")\n",
    "\n",
    "    # Example: Inspect data for one sample ID (replace 'OMDLxxxxx' with a real ID)\n",
    "    example_sample_id = list(loaded_sequences.keys())[0] # Get the first sample ID found\n",
    "    print(f\"\\nData for sample ID '{example_sample_id}':\")\n",
    "    # Pretty print the list of sequence dictionaries for this sample\n",
    "    print(json.dumps(loaded_sequences[example_sample_id], indent=2, default=str)) # Use default=str to handle SeqRecord object if present\n",
    "\n",
    "    # Verify structure of one sequence entry\n",
    "    first_seq_data = loaded_sequences[example_sample_id][0]\n",
    "    print(\"\\nStructure of one sequence entry:\")\n",
    "    print(f\"  Header: {first_seq_data.get('header')[:50]}...\") # Show first 50 chars\n",
    "    print(f\"  Length: {first_seq_data.get('length')}\")\n",
    "    print(f\"  RiC: {first_seq_data.get('ric')}\")\n",
    "    print(f\"  Sequence snippet: {first_seq_data.get('sequence')[:50]}...\") # Show first 50 chars\n",
    "else:\n",
    "    print(f\"Failed to load data for {test_run_id} {test_basecaller}. Check file path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test K-mer matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_a = \"ATGCGATGCGATGCG\"\n",
    "seq_b = \"ATGCGATGCGATGCG\" # Identical\n",
    "seq_c = \"ATGCGATTCGATGCG\" # One mismatch\n",
    "seq_d = \"AAAAAAAAAAAAAAA\" # Different\n",
    "seq_e = \"ATGCG\"             # Too short for k=7\n",
    "seq_f = \"\"                # Empty\n",
    "\n",
    "k_val = 7\n",
    "print(f\"Similarity A vs B (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_b, k=k_val):.2f}%\")\n",
    "print(f\"Similarity A vs C (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_c, k=k_val):.2f}%\") # Test mismatch 22.22%\n",
    "print(f\"Similarity B vs A (k={k_val}): {data_functions.calculate_kmer_similarity(seq_b, seq_a, k=k_val):.2f}%\") # Should be symmetric? Test.\n",
    "print(f\"Similarity A vs D (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_d, k=k_val):.2f}%\") # Test different sequence 0.00%\n",
    "print(f\"Similarity A vs E (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_e, k=k_val):.2f}%\") # Test too short sequence 0.00%\n",
    "print(f\"Similarity A vs F (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_f, k=k_val):.2f}%\") # Test empty sequence 0.00%\n",
    "\n",
    "k_val = 3\n",
    "print(f\"\\nSimilarity A vs C (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_c, k=k_val):.2f}%\") # Test smaller k, 76.92%\n",
    "print(f\"Similarity A vs E (k={k_val}): {data_functions.calculate_kmer_similarity(seq_a, seq_e, k=k_val):.2f}%\") # Should work now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Global Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_a = \"ATGCGATGCGATGCG\"\n",
    "seq_b = \"ATGCGATGCGATGCG\" # Identical\n",
    "seq_c = \"ATGCGATTCGATGCG\" # One mismatch\n",
    "seq_d = \"AAAAAAAAAAAAAAA\" # Different\n",
    "seq_indel = \"ATGCGATG---ATGCG\" # Example with deletion relative to A\n",
    "\n",
    "align_ab = data_functions.align_sequences(seq_a, seq_b)\n",
    "align_ac = data_functions.align_sequences(seq_a, seq_c)\n",
    "align_ad = data_functions.align_sequences(seq_a, seq_d)\n",
    "align_a_indel = data_functions.align_sequences(seq_a, seq_indel)\n",
    "\n",
    "print(\"Alignment A vs B:\")\n",
    "print(json.dumps(align_ab, indent=2, default=str)) # Use default=str to handle alignment obj if needed\n",
    "\n",
    "print(\"\\nAlignment A vs C:\")\n",
    "print(json.dumps(align_ac, indent=2, default=str))\n",
    "\n",
    "print(\"\\nAlignment A vs D:\")\n",
    "print(json.dumps(align_ad, indent=2, default=str))\n",
    "\n",
    "print(\"\\nAlignment A vs Indel:\")\n",
    "print(json.dumps(align_a_indel, indent=2, default=str))\n",
    "\n",
    "# Test empty sequence\n",
    "align_a_empty = data_functions.align_sequences(seq_a, \"\")\n",
    "print(\"\\nAlignment A vs Empty:\")\n",
    "print(align_a_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Matching Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Test Sequences ---\n",
    "seq_a = \"ATGCGATGCGATGCG\"     # Base sequence\n",
    "seq_b = \"ATGCGATGCGATGCG\"     # Identical to A\n",
    "seq_c = \"ATGCGATTCGATGCG\"     # One mismatch vs A\n",
    "seq_d = \"AAAAAAAAAAAAAAA\"     # Very different from A\n",
    "seq_indel = \"ATGCGATG---ATGCG\"  # Contains gaps (Note: align_sequences takes raw seqs, not pre-aligned)\n",
    "seq_e = \"ATGCG\"                 # Short sequence\n",
    "seq_f = \"\"                    # Empty sequence\n",
    "# Create sequences similar to D and C for testing many:many and ambiguous cases\n",
    "seq_d_like = \"AAAAAAAAAAAAAAC\" # Similar to D\n",
    "seq_c_prime = \"ATGCGATTCAATGCG\" # Similar to C (two mismatches vs A)\n",
    "\n",
    "# --- Helper Function to Create Sequence Records ---\n",
    "# Mimics the structure produced by load_sequences\n",
    "def create_record(seq_id: str, sequence: str, ric: int, source: str, sample: str, rep_num: int = 1):\n",
    "    \"\"\"Creates a dictionary representing a sequence record.\"\"\"\n",
    "    # Create a somewhat realistic header based on inputs\n",
    "    header = f\">ONT01.01-{sample}-{seq_id}-iNat0000{rep_num} ric={ric}\"\n",
    "    return {\n",
    "        'header': header,\n",
    "        'sequence': sequence,\n",
    "        'length': len(sequence),\n",
    "        'ric': ric,\n",
    "        'seq_object': None # Placeholder, not needed for matching logic testing\n",
    "    }\n",
    "\n",
    "# --- Create Mock Dorado Sequences Dictionary ---\n",
    "mock_dorado_seqs = {\n",
    "    # Scenario S1: Simple 1:1 High Identity\n",
    "    'S1': [create_record('D_S1_1', seq_a, 100, 'dorado', 'S1')],\n",
    "    # Scenario S2: Simple 1:1 Lower Identity\n",
    "    'S2': [create_record('D_S2_1', seq_a, 90, 'dorado', 'S2')],\n",
    "    # Scenario S3: Unmatched Pair\n",
    "    'S3': [create_record('D_S3_1', seq_a, 80, 'dorado', 'S3')],\n",
    "    # Scenario S4: Dorado Only Sample\n",
    "    'S4': [create_record('D_S4_1', seq_a, 70, 'dorado', 'S4')],\n",
    "    # Scenario S6: 1 Dorado, 2 Guppy\n",
    "    'S6': [create_record('D_S6_1', seq_a, 110, 'dorado', 'S6')],\n",
    "    # Scenario S7: 2 Dorado, 2 Guppy (Clear matches)\n",
    "    'S7': [\n",
    "        create_record('D_S7_1', seq_a, 120, 'dorado', 'S7', rep_num=1),\n",
    "        create_record('D_S7_2', seq_d, 50, 'dorado', 'S7', rep_num=2)\n",
    "    ],\n",
    "    # Scenario S8: 1 Dorado, 2 Guppy (Ambiguous matches)\n",
    "    'S8': [create_record('D_S8_1', seq_a, 130, 'dorado', 'S8')],\n",
    "}\n",
    "\n",
    "# --- Create Mock Guppy Sequences Dictionary ---\n",
    "mock_guppy_seqs = {\n",
    "    # Scenario S1: Simple 1:1 High Identity\n",
    "    'S1': [create_record('G_S1_1', seq_b, 95, 'guppy', 'S1')],\n",
    "    # Scenario S2: Simple 1:1 Lower Identity\n",
    "    'S2': [create_record('G_S2_1', seq_c, 85, 'guppy', 'S2')],\n",
    "    # Scenario S3: Unmatched Pair\n",
    "    'S3': [create_record('G_S3_1', seq_d, 75, 'guppy', 'S3')],\n",
    "    # Scenario S5: Guppy Only Sample\n",
    "    'S5': [create_record('G_S5_1', seq_a, 65, 'guppy', 'S5')],\n",
    "    # Scenario S6: 1 Dorado, 2 Guppy\n",
    "    'S6': [\n",
    "        create_record('G_S6_1', seq_b, 105, 'guppy', 'S6', rep_num=1), # Should match D_S6_1 well\n",
    "        create_record('G_S6_2', seq_c, 45, 'guppy', 'S6', rep_num=2)  # Should match D_S6_1 less well\n",
    "    ],\n",
    "    # Scenario S7: 2 Dorado, 2 Guppy (Clear matches)\n",
    "    'S7': [\n",
    "        create_record('G_S7_1', seq_b, 115, 'guppy', 'S7', rep_num=1), # Should match D_S7_1 (A)\n",
    "        create_record('G_S7_2', seq_d_like, 55, 'guppy', 'S7', rep_num=2) # Should match D_S7_2 (D)\n",
    "    ],\n",
    "    # Scenario S8: 1 Dorado, 2 Guppy (Ambiguous matches)\n",
    "    'S8': [\n",
    "        create_record('G_S8_1', seq_c, 125, 'guppy', 'S8', rep_num=1),       # Similar match to D_S8_1 (A)\n",
    "        create_record('G_S8_2', seq_c_prime, 110, 'guppy', 'S8', rep_num=2) # Also similar match to D_S8_1 (A)\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Mock data dictionaries created: mock_dorado_seqs, mock_guppy_seqs\")\n",
    "# Print a sample entry to verify structure\n",
    "example_sample_id = 'S7'\n",
    "print(f\"\\nExample entry for {example_sample_id} in mock_dorado_seqs:\")\n",
    "print(json.dumps(mock_dorado_seqs.get(example_sample_id, 'Not Found'), indent=2))\n",
    "print(f\"\\nExample entry for {example_sample_id} in mock_guppy_seqs:\")\n",
    "print(json.dumps(mock_guppy_seqs.get(example_sample_id, 'Not Found'), indent=2))\n",
    "\n",
    "matched, dorado_unmatched, guppy_unmatched = data_functions.match_sequences(mock_dorado_seqs, mock_guppy_seqs)\n",
    "\n",
    "print(f\"Matched pairs: {len(matched)}\")\n",
    "print(f\"Dorado-only: {len(dorado_unmatched)}\")\n",
    "print(f\"Guppy-only: {len(guppy_unmatched)}\")\n",
    "\n",
    "print(\"\\n--- Matched Pairs ---\")\n",
    "for pair in matched:\n",
    "    print(f\"  Sample: {pair['sample_id']}, \"\n",
    "          f\"D_Header: {pair['dorado'].get('header','N/A')}, \"\n",
    "          f\"G_Header: {pair['guppy'].get('header','N/A')}, \"\n",
    "          f\"Identity: {pair['alignment']['identity']:.2f}%, \"\n",
    "          f\"Multiple: {pair['multiple_matches']}, \"\n",
    "          f\"Confidence: {pair['match_confidence']}\")\n",
    "\n",
    "print(\"\\n--- Dorado Only ---\")\n",
    "for item in dorado_unmatched:\n",
    "    print(f\"  Sample: {item['sample_id']}, Header: {item['record'].get('header','N/A')}\")\n",
    "\n",
    "print(\"\\n--- Guppy Only ---\")\n",
    "for item in guppy_unmatched:\n",
    "    print(f\"  Sample: {item['sample_id']}, Header: {item['record'].get('header','N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing GC Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for GC content\n",
    "seq1 = \"ATGCATGC\" # Expected GC: 0.5\n",
    "seq2 = \"AAAAATTTTT\" # Expected GC: 0.0\n",
    "seq3 = \"GCGCGCGC\" # Expected GC: 1.0\n",
    "seq4 = \"ATGCNNNNATGC\" # Expected GC: 0.5 (gc_fraction ignores 'N')\n",
    "seq5 = \"\" # Expected: None (or handle as 0.0 if preferred)\n",
    "seq6 = \"ATGC-ATGC\" # Expected GC: 0.5 (gc_fraction handles gaps)\n",
    "\n",
    "print(f\"Sequence: '{seq1}', GC Content: {data_functions.calculate_gc_content(seq1)}\")\n",
    "print(f\"Sequence: '{seq2}', GC Content: {data_functions.calculate_gc_content(seq2)}\")\n",
    "print(f\"Sequence: '{seq3}', GC Content: {data_functions.calculate_gc_content(seq3)}\")\n",
    "print(f\"Sequence: '{seq4}', GC Content: {data_functions.calculate_gc_content(seq4)}\")\n",
    "print(f\"Sequence: '{seq5}', GC Content: {data_functions.calculate_gc_content(seq5)}\")\n",
    "print(f\"Sequence: '{seq6}', GC Content: {data_functions.calculate_gc_content(seq6)}\")\n",
    "\n",
    "# Test with invalid input\n",
    "print(f\"Sequence: {123}, GC Content: {data_functions.calculate_gc_content(123)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Homopolymers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test cases for homopolymer analysis\n",
    "seq_hp1 = \"AAAGGGGGTTTTTTCXXXAAAAA\" # A: 3, 5; G: 5; T: 6; C: 1\n",
    "seq_hp2 = \"ACGTACGT\" # No homopolymers >= 3\n",
    "seq_hp3 = \"AAAAAAAAAAAAAAAAAAAA\" # A: 20\n",
    "seq_hp4 = \"acgtgggggaaaaa\" # g: 5, a: 5 (test case insensitivity)\n",
    "seq_hp5 = \"\" # Empty sequence\n",
    "\n",
    "print(f\"--- Testing sequence: '{seq_hp1}' ---\")\n",
    "print(f\"min_len=5: {json.dumps(data_functions.analyze_homopolymers(seq_hp1, min_len=5), indent=2)}\")\n",
    "# Expected for min_len=5: {'A': [5], 'C': [], 'G': [5], 'T': [6], 'total_count': 3, 'max_len': 6}\n",
    "print(f\"min_len=3: {json.dumps(data_functions.analyze_homopolymers(seq_hp1, min_len=3), indent=2)}\")\n",
    "# Expected for min_len=3: {'A': [3, 5], 'C': [], 'G': [5], 'T': [6], 'total_count': 4, 'max_len': 6}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_hp2}' ---\")\n",
    "print(f\"min_len=3: {json.dumps(data_functions.analyze_homopolymers(seq_hp2, min_len=3), indent=2)}\")\n",
    "# Expected for min_len=3: {'A': [], 'C': [], 'G': [], 'T': [], 'total_count': 0, 'max_len': 0}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_hp3}' ---\")\n",
    "print(f\"min_len=10: {json.dumps(data_functions.analyze_homopolymers(seq_hp3, min_len=10), indent=2)}\")\n",
    "# Expected for min_len=10: {'A': [20], 'C': [], 'G': [], 'T': [], 'total_count': 1, 'max_len': 20}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_hp4}' ---\")\n",
    "print(f\"min_len=4: {json.dumps(data_functions.analyze_homopolymers(seq_hp4, min_len=4), indent=2)}\")\n",
    "# Expected for min_len=4: {'A': [5], 'C': [], 'G': [5], 'T': [], 'total_count': 2, 'max_len': 5}\n",
    "\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_hp5}' ---\")\n",
    "print(f\"min_len=5: {json.dumps(data_functions.analyze_homopolymers(seq_hp5, min_len=5), indent=2)}\")\n",
    "# Expected for min_len=5: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ambiguity Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for ambiguity analysis\n",
    "seq_amb1 = \"ATGCYATGR\" # Y=1, R=1, total=2\n",
    "seq_amb2 = \"ACGTACGT\" # No ambiguity\n",
    "seq_amb3 = \"NNNNNNNNNN\" # N=10, total=10\n",
    "seq_amb4 = \"ATGCnATGCy\" # n=1, y=1, total=2 (test case insensitivity)\n",
    "seq_amb5 = \"\" # Empty sequence\n",
    "\n",
    "print(f\"--- Testing sequence: '{seq_amb1}' ---\")\n",
    "print(json.dumps(data_functions.analyze_ambiguity(seq_amb1), indent=2))\n",
    "# Expected: {'total_count': 2, 'frequency': 0.25, 'counts_per_code': {'Y': 1, 'R': 1}}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_amb2}' ---\")\n",
    "print(json.dumps(data_functions.analyze_ambiguity(seq_amb2), indent=2))\n",
    "# Expected: {'total_count': 0, 'frequency': 0.0, 'counts_per_code': {}}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_amb3}' ---\")\n",
    "print(json.dumps(data_functions.analyze_ambiguity(seq_amb3), indent=2))\n",
    "# Expected: {'total_count': 10, 'frequency': 1.0, 'counts_per_code': {'N': 10}}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_amb4}' ---\")\n",
    "print(json.dumps(data_functions.analyze_ambiguity(seq_amb4), indent=2))\n",
    "# Expected: {'total_count': 2, 'frequency': 0.2, 'counts_per_code': {'N': 1, 'Y': 1}}\n",
    "\n",
    "print(f\"\\n--- Testing sequence: '{seq_amb5}' ---\")\n",
    "print(json.dumps(data_functions.analyze_ambiguity(seq_amb5), indent=2))\n",
    "# Expected: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Match Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'matched' is the list of matched pairs from Step 2.3 testing\n",
    "# If you ran Step 2.3 testing, 'matched' should be available.\n",
    "# If not, you might need to re-run that cell or create a small sample list:\n",
    "# matched_mock = [{'sample_id': 'S1', 'dorado': {...}, 'guppy': {...}, 'alignment': {...}, ...}, ...] # From Step 2.3 output\n",
    "try:\n",
    "     # Check if 'matched' exists from previous steps\n",
    "     if 'matched' in globals() and isinstance(matched, list):\n",
    "          print(f\"Using 'matched' list with {len(matched)} pairs.\")\n",
    "          input_matched_list = matched\n",
    "     else:\n",
    "          # Add fallback or error if 'matched' isn't available\n",
    "          print(\"Warning: 'matched' list not found. Testing with empty list.\")\n",
    "          input_matched_list = []\n",
    "except NameError:\n",
    "     print(\"Warning: 'matched' list not found. Testing with empty list.\")\n",
    "     input_matched_list = []\n",
    "run_comparison_df = data_functions.generate_comparison_dataframe(input_matched_list)\n",
    "\n",
    "print(\"\\nDataFrame Info:\")\n",
    "run_comparison_df.info()\n",
    "\n",
    "print(\"\\nDataFrame Head:\")\n",
    "display(run_comparison_df.head())\n",
    "\n",
    "print(\"\\nDataFrame Description (Numeric columns):\")\n",
    "display(run_comparison_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Statistical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Statistical Wrapper ---\n",
    "print(\"--- Testing perform_paired_nonparametric_test ---\")\n",
    "list1 = [10, 12, 15, 11, 14, 16]\n",
    "list2 = [8, 11, 13, 10, 12, 13] # Generally lower\n",
    "list3 = [10, 12, 15, 11, 14, 16] # Identical to list1\n",
    "list4 = [20, 22, 25, 21, 24, 26] # Generally higher\n",
    "\n",
    "result12 = data_functions.perform_paired_nonparametric_test(list1, list2)\n",
    "result13 = data_functions.perform_paired_nonparametric_test(list1, list3) # Should handle zero differences\n",
    "result14 = data_functions.perform_paired_nonparametric_test(list1, list4)\n",
    "\n",
    "print(f\"Test List1 vs List2: Stat={result12[0] if result12 else 'N/A'}, p={result12[1] if result12 else 'N/A'}\") # Expect potentially significant\n",
    "print(f\"Test List1 vs List3 (Identical): Stat={result13[0] if result13 else 'N/A'}, p={result13[1] if result13 else 'N/A'}\") # Expect p=1.0 or warning\n",
    "print(f\"Test List1 vs List4: Stat={result14[0] if result14 else 'N/A'}, p={result14[1] if result14 else 'N/A'}\") # Expect potentially significant\n",
    "\n",
    "# Test edge cases\n",
    "result_short = data_functions.perform_paired_nonparametric_test([1], [2])\n",
    "result_empty = data_functions.perform_paired_nonparametric_test([], [])\n",
    "result_mismatch = data_functions.perform_paired_nonparametric_test([1, 2], [3])\n",
    "print(f\"Test Short: {result_short}\")\n",
    "print(f\"Test Empty: {result_empty}\")\n",
    "print(f\"Test Mismatch Length: {result_mismatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Run-Specific Statistics ---\n",
    "print(\"\\n--- Testing calculate_run_statistics ---\")\n",
    "\n",
    "# Assume 'run_comparison_df' is available from Step 3.4 for a selected run\n",
    "if 'run_comparison_df' in globals() and isinstance(run_comparison_df, pd.DataFrame) and not run_comparison_df.empty:\n",
    "    run_stats_results = data_functions.calculate_run_statistics(run_comparison_df)\n",
    "\n",
    "    if run_stats_results:\n",
    "        print(\"Successfully calculated run statistics:\")\n",
    "        # Pretty print the results dictionary\n",
    "        print(json.dumps(run_stats_results, indent=2, default=str)) # Use default=str for potential numpy types\n",
    "\n",
    "        # Example: Check RiC results\n",
    "        if 'RiC' in run_stats_results:\n",
    "            print(f\"\\nRiC Median Difference: {run_stats_results['RiC'].get('median_diff')}\")\n",
    "            print(f\"RiC p-value: {run_stats_results['RiC'].get('p_value')}\")\n",
    "    else:\n",
    "        print(\"Failed to calculate run statistics.\")\n",
    "else:\n",
    "    print(\"Skipping test: 'run_comparison_df' not available or empty.\")\n",
    "    # Create a small mock DataFrame for testing if needed\n",
    "    # mock_df = pd.DataFrame({\n",
    "    #      'Dorado_RiC': [10, 12, 15, 11, 14, 16, np.nan],\n",
    "    #      'Guppy_RiC': [8, 11, 13, 10, 12, 13, 9],\n",
    "    #      'Dorado_Length': [100, 102, 105, 101, 104, 106, 100],\n",
    "    #      'Guppy_Length': [98, 101, 103, 100, 102, 103, 99],\n",
    "    #      # Add other columns as needed... ensure they match metric_pairs\n",
    "    # })\n",
    "    # mock_results = data_functions.calculate_run_statistics(mock_df)\n",
    "    # print(\"Mock Results:\")\n",
    "    # print(json.dumps(mock_results, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Run-Specific Output ---\n",
    "print(\"\\n--- Testing save_run_comparison ---\")\n",
    "\n",
    "# Assume 'run_comparison_df' is available from Step 3.4 for a selected run\n",
    "# Assume 'selected_run_id' and 'RESULTS_DIR' are defined\n",
    "test_run_id = 'OMDL_Test' # Use a mock ID or the selected_run_id\n",
    "mock_df_for_saving = run_comparison_df # Or create a small mock DataFrame if needed\n",
    "\n",
    "if 'mock_df_for_saving' in globals() and isinstance(mock_df_for_saving, pd.DataFrame) and not mock_df_for_saving.empty:\n",
    "    # Test saving as TSV\n",
    "    saved_tsv_path = data_functions.save_run_comparison(\n",
    "        mock_df_for_saving,\n",
    "        test_run_id,\n",
    "        RESULTS_DIR,\n",
    "        format='tsv'\n",
    "    )\n",
    "    if saved_tsv_path and os.path.exists(saved_tsv_path):\n",
    "         print(f\"TSV file check successful: {saved_tsv_path}\")\n",
    "         # Optional: Read back file to verify content\n",
    "         # check_df = pd.read_csv(saved_tsv_path, sep='\\t')\n",
    "         # print(f\"Read back {len(check_df)} rows from TSV.\")\n",
    "\n",
    "    # Test saving as CSV\n",
    "    saved_csv_path = data_functions.save_run_comparison(\n",
    "         mock_df_for_saving,\n",
    "         test_run_id,\n",
    "         RESULTS_DIR,\n",
    "         format='csv'\n",
    "    )\n",
    "    if saved_csv_path and os.path.exists(saved_csv_path):\n",
    "         print(f\"CSV file check successful: {saved_csv_path}\")\n",
    "\n",
    "else:\n",
    "     print(\"Skipping test: DataFrame for saving is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Overall Summary Output ---\n",
    "print(\"\\n--- Testing generate_overall_summary ---\")\n",
    "\n",
    "# Assume 'all_runs_analysis_results' is populated by the notebook's main loop\n",
    "# It should look like: {'OMDL1': {'stats': {...}, 'counts': {...}}, 'OMDL2': {...}, ...}\n",
    "\n",
    "# Create a mock results dictionary for testing if needed:\n",
    "mock_all_runs_results = {\n",
    "    'OMDL1': {\n",
    "        'stats': {\n",
    "            'RiC': {'median_diff': 5.0, 'p_value': 0.21, 'n_pairs': 150},\n",
    "            'Length': {'median_diff': -1.0, 'p_value': 0.04, 'n_pairs': 150}\n",
    "            # Add other metrics...\n",
    "        },\n",
    "        'counts': {'matched': 150, 'dorado_only': 5, 'guppy_only': 10}\n",
    "    },\n",
    "    'OMDL2': {\n",
    "         'stats': {\n",
    "            'RiC': {'median_diff': 10.0, 'p_value': 0.001, 'n_pairs': 200},\n",
    "            'Length': {'median_diff': 0.0, 'p_value': 0.95, 'n_pairs': 200}\n",
    "            # Add other metrics...\n",
    "         },\n",
    "         'counts': {'matched': 200, 'dorado_only': 2, 'guppy_only': 3}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test saving as TSV using mock data\n",
    "saved_summary_path = data_functions.generate_overall_summary(\n",
    "    mock_all_runs_results, # Use mock_all_runs_results or the real all_runs_analysis_results\n",
    "    RESULTS_DIR,\n",
    "    format='tsv'\n",
    ")\n",
    "\n",
    "if saved_summary_path and os.path.exists(saved_summary_path):\n",
    "     print(f\"Overall summary file check successful: {saved_summary_path}\")\n",
    "     # Optional: Read back file to verify content\n",
    "     # check_summary_df = pd.read_csv(saved_summary_path, sep='\\t')\n",
    "     # print(\"Overall Summary DataFrame Head:\")\n",
    "     # display(check_summary_df.head())\n",
    "else:\n",
    "     print(\"Failed to save overall summary file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test vizualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the function that displays run analysis (called by dropdown observer)\n",
    "run_id = selected_run_id\n",
    "run_data = all_runs_analysis_results.get(run_id)\n",
    "if run_data and 'comparison_df' in run_data:\n",
    "    run_df = run_data['comparison_df']\n",
    "    if not run_df.empty:\n",
    "        # Example call for RiC comparison\n",
    "        fig_ric, _ = plot_comparison_with_difference(\n",
    "            run_df,\n",
    "            dorado_col='Dorado_RiC',\n",
    "            guppy_col='Guppy_RiC',\n",
    "            diff_col='RiC_Difference',\n",
    "            figure_title=f'{run_id} - RiC Comparison'\n",
    "        )\n",
    "        plt.show(fig_ric) # Display the plot\n",
    "\n",
    "        # Example call for Identity Distribution\n",
    "        fig_identity, _ = plot_histogram(\n",
    "             run_df,\n",
    "             metric_col='Identity_Percent',\n",
    "             title=f'{run_id} - Sequence Identity Distribution',\n",
    "             xlabel='Sequence Identity (%)'\n",
    "        )\n",
    "        plt.show(fig_identity) # Display the plot\n",
    "    else:\n",
    "        print(\"Comparison DataFrame is empty.\")\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
